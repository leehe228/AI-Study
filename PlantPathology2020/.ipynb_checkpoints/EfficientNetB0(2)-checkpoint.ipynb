{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c9daf2a-dfa6-4ea1-9e44-1b15f962814a",
   "metadata": {},
   "source": [
    "# Plant Pathology 2020 - EfficientNetB0\n",
    "https://www.kaggle.com/c/plant-pathology-2020-fgvc7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a8c285-c0aa-459f-ae45-7c5b137d4120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "op = os.path.join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8839e662-2853-471b-aa65-a25ea44cac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0263a678-0a28-4260-9c55-6498f78aacd8",
   "metadata": {},
   "source": [
    "### CUDA GPU Device Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67b37a6f-2f14-42b1-896e-ba70f7908836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17bc27b-4d82-4e8d-8e51-f8eb610aee6b",
   "metadata": {},
   "source": [
    "### Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "661fe7f3-bde5-4a2a-bc90-d3f2a679a527",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/dmsai2/Desktop/AI-Study/PlantPathology2020/\"\n",
    "train_csv = op(path, \"train.csv\")\n",
    "train_path = op(path, \"images\", \"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7cd4b64-ce56-48f7-be85-d1a65e318b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1821\n"
     ]
    }
   ],
   "source": [
    "print(\"train:\", len(os.listdir(train_path)))\n",
    "n_train_data = len(os.listdir(train_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32891f11-618b-451a-a089-d506a38aac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e6cced-7eaa-41e7-91a2-d379c8b68de3",
   "metadata": {},
   "source": [
    "### Transform for DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "873caa9a-4791-4031-b9ad-860026e852c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f4efb52-36ec-44ba-96b1-e3b691823239",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    # transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563f9b7b-d83b-40e3-a5c3-f99ae1a60610",
   "metadata": {},
   "source": [
    "### PyTorch Customized `Datasets` Class\n",
    "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39f8dd8d-6084-4335-a4ae-40703fb1f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2fcebce-13ee-4a14-9bec-faded67e790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantPathologyDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None, header=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path + '.jpg')\n",
    "        image = image.type(torch.FloatTensor)\n",
    "        image = image / 255.\n",
    "    \n",
    "        label = np.argmax(self.img_labels.iloc[idx, 1:].values)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b8f3a3-072d-43a5-af2a-8262c1846b8e",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49bd803c-70b4-4e2f-a31d-385bdb8ccc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PlantPathologyDataset(train_csv, train_path, transform=tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a272d77-ea17-4411-b4bc-b2c3c825f159",
   "metadata": {},
   "source": [
    "### Show Image Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f60ffbd2-8908-4eae-93a2-6472283b529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(data, is_test=False):\n",
    "    f, ax = plt.subplots(5, 5, figsize=(15, 10))\n",
    "    \n",
    "    for i in range(25):\n",
    "        img_dir = data.img_labels.iloc[i, 0]\n",
    "        img_data = cv2.imread(op(train_path, img_dir + '.jpg'))\n",
    "        label = np.argmax(data.img_labels.iloc[0, 1:].values)\n",
    "        \n",
    "        if label  == 0:  str_label = 'healthy'\n",
    "        elif label == 1:  str_label = 'multiple_diseases'\n",
    "        elif label == 2: str_label = 'rust'\n",
    "        else: str_label = 'scab'\n",
    "        if(is_test): str_label=\"None\"\n",
    "        \n",
    "        ax[i//5, i%5].imshow(img_data)\n",
    "        ax[i//5, i%5].axis('off')\n",
    "        ax[i//5, i%5].set_title(\"Label: {}\".format(str_label))\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "515b521c-c41f-48fa-b57d-dab26230a15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_images(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7610afc-8d86-4348-a0bd-c1d559d76be0",
   "metadata": {},
   "source": [
    "### Train Test Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70b89da6-72ae-4e03-850c-67d62864ca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * 0.8)\n",
    "test_size = dataset_size - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a89d152-4d7d-46c7-84a0-58debcbd0b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73965efa-ad2c-48a1-894c-aca1fdbdb0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size : 1456\n",
      "Testing Data Size : 365\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Data Size : {len(train_dataset)}\")\n",
    "print(f\"Testing Data Size : {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edffddc-a696-4997-8cd6-0dba37a7125c",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ecbb1c7-f0ad-42c7-95fb-b57d81f3d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f5987f6-be47-404e-aa79-d9aaca4a75bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              shuffle=True, \n",
    "                              drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             batch_size=TEST_BATCH_SIZE, \n",
    "                             shuffle=True, \n",
    "                             drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "656ab8b0-0503-4650-b523-9c675f4284e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/hackthon2/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 3, 224, 224])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {X_train.size()}\")\n",
    "print(f\"Labels batch shape: {y_train.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d84243-a8b7-4b88-a389-4d9714484c35",
   "metadata": {},
   "source": [
    "### Activation Function (Swish based ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c29565a0-e61a-4819-b95e-25f51b1c0cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_fn(x):\n",
    "    \"\"\" Swish activation function \"\"\"\n",
    "    return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda0ffaa-8d90-4f23-9cd8-3adf1aa547d0",
   "metadata": {},
   "source": [
    "### EfficientNetB0 Model\n",
    "https://startnow95.tistory.com/4 <br>\n",
    "https://deep-learning-study.tistory.com/563"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31edd4d5-7d2d-424a-8875-f3e8636d4696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fb0211-4ac8-4b01-b682-edeebdad7467",
   "metadata": {},
   "source": [
    "class Conv2dSamePadding(nn.Conv2d):\n",
    "    \"\"\" 2D Convolutions like TensorFlow \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ih, iw = x.size()[-2:]\n",
    "        kh, kw = self.weight.size()[-2:]\n",
    "        sh, sw = self.stride\n",
    "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
    "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
    "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            x = F.pad(x, [pad_w//2, pad_w - pad_w//2, pad_h//2, pad_h - pad_h//2])\n",
    "        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2205329c-8672-44b3-b96d-5ebfe8534386",
   "metadata": {},
   "source": [
    "def drop_connect(inputs, p, training):\n",
    "    \"\"\" Drop connect. \"\"\"\n",
    "    if not training: return inputs\n",
    "    batch_size = inputs.shape[0]\n",
    "    keep_prob = 1 - p\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype)  # uniform [0,1)\n",
    "    binary_tensor = torch.floor(random_tensor)\n",
    "    output = inputs / keep_prob * binary_tensor\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676f4e4a-0b74-475a-8510-1403f04cfc32",
   "metadata": {},
   "source": [
    "class MBConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Mobile Inverted Residual Bottleneck Block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size, stride, expand_ratio, input_filters, output_filters, se_ratio, drop_n_add):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._bn_mom = 0.1\n",
    "        self._bn_eps = 1e-03\n",
    "        self.has_se = (se_ratio is not None) and (0 < se_ratio <= 1)\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.drop_n_add = drop_n_add\n",
    "\n",
    "        # Filter Expansion phase\n",
    "        inp = input_filters  # number of input channels\n",
    "        oup = input_filters * expand_ratio  # number of output channels\n",
    "        if expand_ratio != 1: # add it except at first block \n",
    "            self._expand_conv = Conv2dSamePadding(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n",
    "            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "\n",
    "        # Depthwise convolution phase\n",
    "        k = kernel_size\n",
    "        s = stride\n",
    "        self._depthwise_conv = Conv2dSamePadding(\n",
    "            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise(conv filter by filter)\n",
    "            kernel_size=k, stride=s, bias=False)\n",
    "        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "\n",
    "        # Squeeze and Excitation layer, if desired\n",
    "        if self.has_se:\n",
    "            num_squeezed_channels = max(1,int(input_filters * se_ratio))  # input channel * 0.25 ex) block2 => 16 * 0.25 = 4\n",
    "            self._se_reduce = Conv2dSamePadding(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n",
    "            self._se_expand = Conv2dSamePadding(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n",
    "\n",
    "        # Output phase\n",
    "        final_oup = output_filters\n",
    "        self._project_conv = Conv2dSamePadding(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n",
    "        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "        \n",
    "    def forward(self, inputs, drop_connect_rate=0.2):\n",
    "\n",
    "        # Expansion and Depthwise Convolution\n",
    "        x = inputs.to(device)\n",
    "        if self.expand_ratio != 1:\n",
    "            x = relu_fn(self._bn0(self._expand_conv(inputs)))\n",
    "        x = relu_fn(self._bn1(self._depthwise_conv(x)))\n",
    "\n",
    "        # Squeeze and Excitation\n",
    "        if self.has_se:\n",
    "            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n",
    "            x_squeezed = self._se_expand(relu_fn(self._se_reduce(x_squeezed)))\n",
    "            x = torch.sigmoid(x_squeezed) * x\n",
    "            \n",
    "        # Output phase\n",
    "        x = self._bn2(self._project_conv(x))\n",
    "\n",
    "        # Skip connection and drop connect\n",
    "        if self.drop_n_add == True:\n",
    "            if drop_connect_rate:\n",
    "                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n",
    "            x = x + inputs  # skip connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bfeb67-bceb-4e33-a0dc-332910135402",
   "metadata": {},
   "source": [
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Batch norm parameters\n",
    "        bn_mom = 0.1\n",
    "        bn_eps = 1e-03\n",
    "\n",
    "        # stem\n",
    "        in_channels = 3\n",
    "        out_channels = 32\n",
    "        self._conv_stem = Conv2dSamePadding(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n",
    "        self._bn0 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
    "\n",
    "        # Build blocks\n",
    "        self._blocks = nn.ModuleList([]) # list 형태로 model 구성할 때\n",
    "        # stage2 r1_k3_s11_e1_i32_o16_se0.25\n",
    "        self._blocks.append(MBConvBlock(kernel_size=3, stride=1, expand_ratio=1, input_filters=32, output_filters=16, se_ratio=0.25, drop_n_add=False))\n",
    "        # stage3 r2_k3_s22_e6_i16_o24_se0.25\n",
    "        self._blocks.append(MBConvBlock(3, 2, 6, 16, 24, 0.25, False))\n",
    "        self._blocks.append(MBConvBlock(3, 1, 6, 24, 24, 0.25, True))\n",
    "        # stage4 r2_k5_s22_e6_i24_o40_se0.25\n",
    "        self._blocks.append(MBConvBlock(5, 2, 6, 24, 40, 0.25, False))\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 40, 40, 0.25, True))\n",
    "        # stage5 r3_k3_s22_e6_i40_o80_se0.25\n",
    "        self._blocks.append(MBConvBlock(3, 2, 6, 40, 80, 0.25, False))\n",
    "        self._blocks.append(MBConvBlock(3, 1, 6, 80, 80, 0.25, True))\n",
    "        self._blocks.append(MBConvBlock(3, 1, 6, 80, 80, 0.25, True))\n",
    "        # stage6 r3_k5_s11_e6_i80_o112_se0.25\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 80,  112, 0.25, False))\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 112, 112, 0.25, True))\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 112, 112, 0.25, True))\n",
    "        # stage7 r4_k5_s22_e6_i112_o192_se0.25\n",
    "        self._blocks.append(MBConvBlock(5, 2, 6, 112, 192, 0.25, False))\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 192, 192, 0.25, True))\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 192, 192, 0.25, True))\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 192, 192, 0.25, True))\n",
    "        # stage8 r1_k3_s11_e6_i192_o320_se0.25\n",
    "        self._blocks.append(MBConvBlock(3, 1, 6, 192, 320, 0.25, False))\n",
    "\n",
    "        # Head \n",
    "        in_channels = 320\n",
    "        out_channels = 1280\n",
    "        self._conv_head = Conv2dSamePadding(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self._bn1 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
    "\n",
    "        # Final linear layer\n",
    "        self._dropout = 0.2\n",
    "        self._num_classes = 10\n",
    "        self._fc = nn.Linear(out_channels, self._num_classes)\n",
    "\n",
    "    def extract_features(self, inputs):\n",
    "        \"\"\" Returns output of the final convolution layer \"\"\"\n",
    "\n",
    "        # Stem\n",
    "        x = relu_fn(self._bn0(self._conv_stem(inputs)))\n",
    "\n",
    "        # Blocks\n",
    "        for idx, block in enumerate(self._blocks):          \n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Calls extract_features to extract features, applies final linear layer, and returns logits. \"\"\"\n",
    "        \n",
    "        # Convolution layers\n",
    "        x = self.extract_features(inputs).to(device)\n",
    "\n",
    "        # Head\n",
    "        x = relu_fn(self._bn1(self._conv_head(x)))\n",
    "        x = F.adaptive_avg_pool2d(x, 1).squeeze(-1).squeeze(-1)\n",
    "        if self._dropout:\n",
    "            x = F.dropout(x, p=self._dropout, training=self.training)\n",
    "        x = self._fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f480064-6f9e-4464-918f-bbf9200b7dbd",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db7d08ed-e592-4eb7-83fb-f60bec907785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchsummary\n",
    "from torchsummary import summary as summary_\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a5854ed-b093-4022-84c0-d003961d0515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "937e0392-92fa-4fb8-a069-4417aa6409d6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         ZeroPad2d-1           [2, 3, 225, 225]               0\n",
      "Conv2dStaticSamePadding-2          [2, 32, 112, 112]             864\n",
      "       BatchNorm2d-3          [2, 32, 112, 112]              64\n",
      "MemoryEfficientSwish-4          [2, 32, 112, 112]               0\n",
      "         ZeroPad2d-5          [2, 32, 114, 114]               0\n",
      "Conv2dStaticSamePadding-6          [2, 32, 112, 112]             288\n",
      "       BatchNorm2d-7          [2, 32, 112, 112]              64\n",
      "MemoryEfficientSwish-8          [2, 32, 112, 112]               0\n",
      "          Identity-9              [2, 32, 1, 1]               0\n",
      "Conv2dStaticSamePadding-10               [2, 8, 1, 1]             264\n",
      "MemoryEfficientSwish-11               [2, 8, 1, 1]               0\n",
      "         Identity-12               [2, 8, 1, 1]               0\n",
      "Conv2dStaticSamePadding-13              [2, 32, 1, 1]             288\n",
      "         Identity-14          [2, 32, 112, 112]               0\n",
      "Conv2dStaticSamePadding-15          [2, 16, 112, 112]             512\n",
      "      BatchNorm2d-16          [2, 16, 112, 112]              32\n",
      "      MBConvBlock-17          [2, 16, 112, 112]               0\n",
      "         Identity-18          [2, 16, 112, 112]               0\n",
      "Conv2dStaticSamePadding-19          [2, 96, 112, 112]           1,536\n",
      "      BatchNorm2d-20          [2, 96, 112, 112]             192\n",
      "MemoryEfficientSwish-21          [2, 96, 112, 112]               0\n",
      "        ZeroPad2d-22          [2, 96, 113, 113]               0\n",
      "Conv2dStaticSamePadding-23            [2, 96, 56, 56]             864\n",
      "      BatchNorm2d-24            [2, 96, 56, 56]             192\n",
      "MemoryEfficientSwish-25            [2, 96, 56, 56]               0\n",
      "         Identity-26              [2, 96, 1, 1]               0\n",
      "Conv2dStaticSamePadding-27               [2, 4, 1, 1]             388\n",
      "MemoryEfficientSwish-28               [2, 4, 1, 1]               0\n",
      "         Identity-29               [2, 4, 1, 1]               0\n",
      "Conv2dStaticSamePadding-30              [2, 96, 1, 1]             480\n",
      "         Identity-31            [2, 96, 56, 56]               0\n",
      "Conv2dStaticSamePadding-32            [2, 24, 56, 56]           2,304\n",
      "      BatchNorm2d-33            [2, 24, 56, 56]              48\n",
      "      MBConvBlock-34            [2, 24, 56, 56]               0\n",
      "         Identity-35            [2, 24, 56, 56]               0\n",
      "Conv2dStaticSamePadding-36           [2, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-37           [2, 144, 56, 56]             288\n",
      "MemoryEfficientSwish-38           [2, 144, 56, 56]               0\n",
      "        ZeroPad2d-39           [2, 144, 58, 58]               0\n",
      "Conv2dStaticSamePadding-40           [2, 144, 56, 56]           1,296\n",
      "      BatchNorm2d-41           [2, 144, 56, 56]             288\n",
      "MemoryEfficientSwish-42           [2, 144, 56, 56]               0\n",
      "         Identity-43             [2, 144, 1, 1]               0\n",
      "Conv2dStaticSamePadding-44               [2, 6, 1, 1]             870\n",
      "MemoryEfficientSwish-45               [2, 6, 1, 1]               0\n",
      "         Identity-46               [2, 6, 1, 1]               0\n",
      "Conv2dStaticSamePadding-47             [2, 144, 1, 1]           1,008\n",
      "         Identity-48           [2, 144, 56, 56]               0\n",
      "Conv2dStaticSamePadding-49            [2, 24, 56, 56]           3,456\n",
      "      BatchNorm2d-50            [2, 24, 56, 56]              48\n",
      "      MBConvBlock-51            [2, 24, 56, 56]               0\n",
      "         Identity-52            [2, 24, 56, 56]               0\n",
      "Conv2dStaticSamePadding-53           [2, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-54           [2, 144, 56, 56]             288\n",
      "MemoryEfficientSwish-55           [2, 144, 56, 56]               0\n",
      "        ZeroPad2d-56           [2, 144, 59, 59]               0\n",
      "Conv2dStaticSamePadding-57           [2, 144, 28, 28]           3,600\n",
      "      BatchNorm2d-58           [2, 144, 28, 28]             288\n",
      "MemoryEfficientSwish-59           [2, 144, 28, 28]               0\n",
      "         Identity-60             [2, 144, 1, 1]               0\n",
      "Conv2dStaticSamePadding-61               [2, 6, 1, 1]             870\n",
      "MemoryEfficientSwish-62               [2, 6, 1, 1]               0\n",
      "         Identity-63               [2, 6, 1, 1]               0\n",
      "Conv2dStaticSamePadding-64             [2, 144, 1, 1]           1,008\n",
      "         Identity-65           [2, 144, 28, 28]               0\n",
      "Conv2dStaticSamePadding-66            [2, 40, 28, 28]           5,760\n",
      "      BatchNorm2d-67            [2, 40, 28, 28]              80\n",
      "      MBConvBlock-68            [2, 40, 28, 28]               0\n",
      "         Identity-69            [2, 40, 28, 28]               0\n",
      "Conv2dStaticSamePadding-70           [2, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-71           [2, 240, 28, 28]             480\n",
      "MemoryEfficientSwish-72           [2, 240, 28, 28]               0\n",
      "        ZeroPad2d-73           [2, 240, 32, 32]               0\n",
      "Conv2dStaticSamePadding-74           [2, 240, 28, 28]           6,000\n",
      "      BatchNorm2d-75           [2, 240, 28, 28]             480\n",
      "MemoryEfficientSwish-76           [2, 240, 28, 28]               0\n",
      "         Identity-77             [2, 240, 1, 1]               0\n",
      "Conv2dStaticSamePadding-78              [2, 10, 1, 1]           2,410\n",
      "MemoryEfficientSwish-79              [2, 10, 1, 1]               0\n",
      "         Identity-80              [2, 10, 1, 1]               0\n",
      "Conv2dStaticSamePadding-81             [2, 240, 1, 1]           2,640\n",
      "         Identity-82           [2, 240, 28, 28]               0\n",
      "Conv2dStaticSamePadding-83            [2, 40, 28, 28]           9,600\n",
      "      BatchNorm2d-84            [2, 40, 28, 28]              80\n",
      "      MBConvBlock-85            [2, 40, 28, 28]               0\n",
      "         Identity-86            [2, 40, 28, 28]               0\n",
      "Conv2dStaticSamePadding-87           [2, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-88           [2, 240, 28, 28]             480\n",
      "MemoryEfficientSwish-89           [2, 240, 28, 28]               0\n",
      "        ZeroPad2d-90           [2, 240, 29, 29]               0\n",
      "Conv2dStaticSamePadding-91           [2, 240, 14, 14]           2,160\n",
      "      BatchNorm2d-92           [2, 240, 14, 14]             480\n",
      "MemoryEfficientSwish-93           [2, 240, 14, 14]               0\n",
      "         Identity-94             [2, 240, 1, 1]               0\n",
      "Conv2dStaticSamePadding-95              [2, 10, 1, 1]           2,410\n",
      "MemoryEfficientSwish-96              [2, 10, 1, 1]               0\n",
      "         Identity-97              [2, 10, 1, 1]               0\n",
      "Conv2dStaticSamePadding-98             [2, 240, 1, 1]           2,640\n",
      "         Identity-99           [2, 240, 14, 14]               0\n",
      "Conv2dStaticSamePadding-100            [2, 80, 14, 14]          19,200\n",
      "     BatchNorm2d-101            [2, 80, 14, 14]             160\n",
      "     MBConvBlock-102            [2, 80, 14, 14]               0\n",
      "        Identity-103            [2, 80, 14, 14]               0\n",
      "Conv2dStaticSamePadding-104           [2, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-105           [2, 480, 14, 14]             960\n",
      "MemoryEfficientSwish-106           [2, 480, 14, 14]               0\n",
      "       ZeroPad2d-107           [2, 480, 16, 16]               0\n",
      "Conv2dStaticSamePadding-108           [2, 480, 14, 14]           4,320\n",
      "     BatchNorm2d-109           [2, 480, 14, 14]             960\n",
      "MemoryEfficientSwish-110           [2, 480, 14, 14]               0\n",
      "        Identity-111             [2, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-112              [2, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-113              [2, 20, 1, 1]               0\n",
      "        Identity-114              [2, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-115             [2, 480, 1, 1]          10,080\n",
      "        Identity-116           [2, 480, 14, 14]               0\n",
      "Conv2dStaticSamePadding-117            [2, 80, 14, 14]          38,400\n",
      "     BatchNorm2d-118            [2, 80, 14, 14]             160\n",
      "     MBConvBlock-119            [2, 80, 14, 14]               0\n",
      "        Identity-120            [2, 80, 14, 14]               0\n",
      "Conv2dStaticSamePadding-121           [2, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-122           [2, 480, 14, 14]             960\n",
      "MemoryEfficientSwish-123           [2, 480, 14, 14]               0\n",
      "       ZeroPad2d-124           [2, 480, 16, 16]               0\n",
      "Conv2dStaticSamePadding-125           [2, 480, 14, 14]           4,320\n",
      "     BatchNorm2d-126           [2, 480, 14, 14]             960\n",
      "MemoryEfficientSwish-127           [2, 480, 14, 14]               0\n",
      "        Identity-128             [2, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-129              [2, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-130              [2, 20, 1, 1]               0\n",
      "        Identity-131              [2, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-132             [2, 480, 1, 1]          10,080\n",
      "        Identity-133           [2, 480, 14, 14]               0\n",
      "Conv2dStaticSamePadding-134            [2, 80, 14, 14]          38,400\n",
      "     BatchNorm2d-135            [2, 80, 14, 14]             160\n",
      "     MBConvBlock-136            [2, 80, 14, 14]               0\n",
      "        Identity-137            [2, 80, 14, 14]               0\n",
      "Conv2dStaticSamePadding-138           [2, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-139           [2, 480, 14, 14]             960\n",
      "MemoryEfficientSwish-140           [2, 480, 14, 14]               0\n",
      "       ZeroPad2d-141           [2, 480, 18, 18]               0\n",
      "Conv2dStaticSamePadding-142           [2, 480, 14, 14]          12,000\n",
      "     BatchNorm2d-143           [2, 480, 14, 14]             960\n",
      "MemoryEfficientSwish-144           [2, 480, 14, 14]               0\n",
      "        Identity-145             [2, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-146              [2, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-147              [2, 20, 1, 1]               0\n",
      "        Identity-148              [2, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-149             [2, 480, 1, 1]          10,080\n",
      "        Identity-150           [2, 480, 14, 14]               0\n",
      "Conv2dStaticSamePadding-151           [2, 112, 14, 14]          53,760\n",
      "     BatchNorm2d-152           [2, 112, 14, 14]             224\n",
      "     MBConvBlock-153           [2, 112, 14, 14]               0\n",
      "        Identity-154           [2, 112, 14, 14]               0\n",
      "Conv2dStaticSamePadding-155           [2, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-156           [2, 672, 14, 14]           1,344\n",
      "MemoryEfficientSwish-157           [2, 672, 14, 14]               0\n",
      "       ZeroPad2d-158           [2, 672, 18, 18]               0\n",
      "Conv2dStaticSamePadding-159           [2, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-160           [2, 672, 14, 14]           1,344\n",
      "MemoryEfficientSwish-161           [2, 672, 14, 14]               0\n",
      "        Identity-162             [2, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-163              [2, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-164              [2, 28, 1, 1]               0\n",
      "        Identity-165              [2, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-166             [2, 672, 1, 1]          19,488\n",
      "        Identity-167           [2, 672, 14, 14]               0\n",
      "Conv2dStaticSamePadding-168           [2, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-169           [2, 112, 14, 14]             224\n",
      "     MBConvBlock-170           [2, 112, 14, 14]               0\n",
      "        Identity-171           [2, 112, 14, 14]               0\n",
      "Conv2dStaticSamePadding-172           [2, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-173           [2, 672, 14, 14]           1,344\n",
      "MemoryEfficientSwish-174           [2, 672, 14, 14]               0\n",
      "       ZeroPad2d-175           [2, 672, 18, 18]               0\n",
      "Conv2dStaticSamePadding-176           [2, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-177           [2, 672, 14, 14]           1,344\n",
      "MemoryEfficientSwish-178           [2, 672, 14, 14]               0\n",
      "        Identity-179             [2, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-180              [2, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-181              [2, 28, 1, 1]               0\n",
      "        Identity-182              [2, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-183             [2, 672, 1, 1]          19,488\n",
      "        Identity-184           [2, 672, 14, 14]               0\n",
      "Conv2dStaticSamePadding-185           [2, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-186           [2, 112, 14, 14]             224\n",
      "     MBConvBlock-187           [2, 112, 14, 14]               0\n",
      "        Identity-188           [2, 112, 14, 14]               0\n",
      "Conv2dStaticSamePadding-189           [2, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-190           [2, 672, 14, 14]           1,344\n",
      "MemoryEfficientSwish-191           [2, 672, 14, 14]               0\n",
      "       ZeroPad2d-192           [2, 672, 17, 17]               0\n",
      "Conv2dStaticSamePadding-193             [2, 672, 7, 7]          16,800\n",
      "     BatchNorm2d-194             [2, 672, 7, 7]           1,344\n",
      "MemoryEfficientSwish-195             [2, 672, 7, 7]               0\n",
      "        Identity-196             [2, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-197              [2, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-198              [2, 28, 1, 1]               0\n",
      "        Identity-199              [2, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-200             [2, 672, 1, 1]          19,488\n",
      "        Identity-201             [2, 672, 7, 7]               0\n",
      "Conv2dStaticSamePadding-202             [2, 192, 7, 7]         129,024\n",
      "     BatchNorm2d-203             [2, 192, 7, 7]             384\n",
      "     MBConvBlock-204             [2, 192, 7, 7]               0\n",
      "        Identity-205             [2, 192, 7, 7]               0\n",
      "Conv2dStaticSamePadding-206            [2, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-207            [2, 1152, 7, 7]           2,304\n",
      "MemoryEfficientSwish-208            [2, 1152, 7, 7]               0\n",
      "       ZeroPad2d-209          [2, 1152, 11, 11]               0\n",
      "Conv2dStaticSamePadding-210            [2, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-211            [2, 1152, 7, 7]           2,304\n",
      "MemoryEfficientSwish-212            [2, 1152, 7, 7]               0\n",
      "        Identity-213            [2, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-214              [2, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-215              [2, 48, 1, 1]               0\n",
      "        Identity-216              [2, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-217            [2, 1152, 1, 1]          56,448\n",
      "        Identity-218            [2, 1152, 7, 7]               0\n",
      "Conv2dStaticSamePadding-219             [2, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-220             [2, 192, 7, 7]             384\n",
      "     MBConvBlock-221             [2, 192, 7, 7]               0\n",
      "        Identity-222             [2, 192, 7, 7]               0\n",
      "Conv2dStaticSamePadding-223            [2, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-224            [2, 1152, 7, 7]           2,304\n",
      "MemoryEfficientSwish-225            [2, 1152, 7, 7]               0\n",
      "       ZeroPad2d-226          [2, 1152, 11, 11]               0\n",
      "Conv2dStaticSamePadding-227            [2, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-228            [2, 1152, 7, 7]           2,304\n",
      "MemoryEfficientSwish-229            [2, 1152, 7, 7]               0\n",
      "        Identity-230            [2, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-231              [2, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-232              [2, 48, 1, 1]               0\n",
      "        Identity-233              [2, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-234            [2, 1152, 1, 1]          56,448\n",
      "        Identity-235            [2, 1152, 7, 7]               0\n",
      "Conv2dStaticSamePadding-236             [2, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-237             [2, 192, 7, 7]             384\n",
      "     MBConvBlock-238             [2, 192, 7, 7]               0\n",
      "        Identity-239             [2, 192, 7, 7]               0\n",
      "Conv2dStaticSamePadding-240            [2, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-241            [2, 1152, 7, 7]           2,304\n",
      "MemoryEfficientSwish-242            [2, 1152, 7, 7]               0\n",
      "       ZeroPad2d-243          [2, 1152, 11, 11]               0\n",
      "Conv2dStaticSamePadding-244            [2, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-245            [2, 1152, 7, 7]           2,304\n",
      "MemoryEfficientSwish-246            [2, 1152, 7, 7]               0\n",
      "        Identity-247            [2, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-248              [2, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-249              [2, 48, 1, 1]               0\n",
      "        Identity-250              [2, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-251            [2, 1152, 1, 1]          56,448\n",
      "        Identity-252            [2, 1152, 7, 7]               0\n",
      "Conv2dStaticSamePadding-253             [2, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-254             [2, 192, 7, 7]             384\n",
      "     MBConvBlock-255             [2, 192, 7, 7]               0\n",
      "        Identity-256             [2, 192, 7, 7]               0\n",
      "Conv2dStaticSamePadding-257            [2, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-258            [2, 1152, 7, 7]           2,304\n",
      "MemoryEfficientSwish-259            [2, 1152, 7, 7]               0\n",
      "       ZeroPad2d-260            [2, 1152, 9, 9]               0\n",
      "Conv2dStaticSamePadding-261            [2, 1152, 7, 7]          10,368\n",
      "     BatchNorm2d-262            [2, 1152, 7, 7]           2,304\n",
      "MemoryEfficientSwish-263            [2, 1152, 7, 7]               0\n",
      "        Identity-264            [2, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-265              [2, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-266              [2, 48, 1, 1]               0\n",
      "        Identity-267              [2, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-268            [2, 1152, 1, 1]          56,448\n",
      "        Identity-269            [2, 1152, 7, 7]               0\n",
      "Conv2dStaticSamePadding-270             [2, 320, 7, 7]         368,640\n",
      "     BatchNorm2d-271             [2, 320, 7, 7]             640\n",
      "     MBConvBlock-272             [2, 320, 7, 7]               0\n",
      "        Identity-273             [2, 320, 7, 7]               0\n",
      "Conv2dStaticSamePadding-274            [2, 1280, 7, 7]         409,600\n",
      "     BatchNorm2d-275            [2, 1280, 7, 7]           2,560\n",
      "MemoryEfficientSwish-276            [2, 1280, 7, 7]               0\n",
      "AdaptiveAvgPool2d-277            [2, 1280, 1, 1]               0\n",
      "         Dropout-278                  [2, 1280]               0\n",
      "          Linear-279                     [2, 4]           5,124\n",
      "================================================================\n",
      "Total params: 4,012,672\n",
      "Trainable params: 4,012,672\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.15\n",
      "Forward/backward pass size (MB): 425.58\n",
      "Params size (MB): 15.31\n",
      "Estimated Total Size (MB): 442.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet.from_name('efficientnet-b0', \n",
    "                               num_classes=4,\n",
    "                               dropout_rate=0.25).to(device)\n",
    "summary_(model, (3, 224, 224), batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f556bb4-c2c8-4daf-aaea-4ebae2f2f3ca",
   "metadata": {},
   "source": [
    "### Neptune AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6f918a4-4376-48ce-a074-6fdca51b3553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06521e48-9c2d-4b8c-b88b-ba7bec2926e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1989052/590607257.py:1: NeptuneWarning: To avoid unintended consumption of logging hours during interactive sessions, the following monitoring options are disabled unless set to 'True' when initializing the run: 'capture_stdout', 'capture_stderr', and 'capture_hardware_metrics'.\n",
      "  run = neptune.init_run(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/leehe228/plant-pathology/e/P001-40\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init_run(\n",
    "    project=\"leehe228/plant-pathology\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI5MTRmYjRlNC0zODFlLTQ0ODItODY1MC1hZGQ0YTRhNDNlZjIifQ==\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eae25d-b0f7-411b-96d5-9cd4b7d2d4c0",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae81c6eb-0dd0-4221-8eb8-080d0c32dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # lr 1e-3 ~ 1e-6\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"input_size\": 3 * 244 * 244,\n",
    "    \"num_epoch\": 25,\n",
    "    \"n_classes\": 4,\n",
    "    \"optimizer\": 'Adam',\n",
    "    \"criterion\": 'CrossEntropyLoss',\n",
    "    \"preproc_type\": \"transform\",\n",
    "    \"model\":\"EfficientNetB0\",\n",
    "    \"library\":\"PyTorch\",\n",
    "    \"dropout\":0.25\n",
    "    \"device\": str(device)\n",
    "}\n",
    "# add parameters\n",
    "run[\"parameters\"] = params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071267ce-8839-4fe0-a488-49d80c8ecc5f",
   "metadata": {},
   "source": [
    "### Optimizer and Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f55a9338-e73a-484e-9459-a4b1413e0c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1289103e-4fea-462f-a447-6e129fb1465c",
   "metadata": {},
   "source": [
    "### Metric: ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c225bd0-aff1-4811-85b3-905203006dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics.aggregation.auc import AUC\n",
    "from torchmetrics import AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "516a1301-82ef-4d10-9c9c-3ae4fd85a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric_auc = AUC()\n",
    "metric_auc = AUROC(task=\"multiclass\", num_classes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125054e5-e8a8-4fe0-b7b4-0c2c8bd1807c",
   "metadata": {},
   "source": [
    "### Learning Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bbaf0b37-38b4-4e71-8689-3f8b6ba3171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T_max : number of iter\n",
    "# eta_min : min value of learning rate\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, \n",
    "#                                                        T_max=5, \n",
    "#                                                        eta_min=1e-6,\n",
    "#                                                        last_epoch=-1,\n",
    "#                                                        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3023b84e-dc70-4b9d-b88b-065ad746d3c9",
   "metadata": {},
   "source": [
    "### Training Funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e0c049c-ab50-4c48-8eee-5af428cfc2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8aa6652-7add-4773-952d-96bf3876579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, params, criterion, optimizer):\n",
    "    \n",
    "    optim_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    run[\"train/epoch/lr\"].append(optim_lr)\n",
    "        \n",
    "    for epoch in range(0, params['num_epoch']):\n",
    "        print(f\"epoch {epoch + 1}.\")\n",
    "        \n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            # data, label 분리\n",
    "            x_train, y_train = data\n",
    "            \n",
    "            # y_train to one_hot_encoding\n",
    "            labels = F.one_hot(y_train, num_classes=4).double()\n",
    "        \n",
    "            inputs = x_train.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # 이전 batch에서 계산된 가중치 초기화\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + back propagation\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            # calculate loss, acc, roc auc\n",
    "            train_loss = criterion(outputs, labels)\n",
    "            train_acc = (torch.sum(preds == torch.argmax(labels, dim=1))).sum().item() / len(inputs) \n",
    "            train_auc = metric_auc(labels, preds).item()\n",
    "            \n",
    "            print(f\"batch {i}/{n_train_data//BATCH_SIZE} acc:{train_acc}, auc:{train_auc}, loss:{train_loss}\")\n",
    "            \n",
    "            # training batch loss and accuracy, auc\n",
    "            run[\"train/batch/loss\"].append(train_loss)\n",
    "            run[\"train/batch/acc\"].append(train_acc)\n",
    "            run['train/batch/auc'].append(train_auc)\n",
    "            \n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            metric_auc.reset()\n",
    "        \n",
    "        # empty GPU RAM\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # test accuracy\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        accuracy = []\n",
    "        auc = []\n",
    "        losses = []\n",
    "        \n",
    "        for i, data in enumerate(test_dataloader, 0):\n",
    "            x_train, y_train = data\n",
    "            \n",
    "            # y_train to one_hot_encoding\n",
    "            labels = F.one_hot(y_train, num_classes=4).double()\n",
    "        \n",
    "            inputs = x_train.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (torch.sum(preds == torch.argmax(labels, dim=1))).sum().item()\n",
    "            test_loss = criterion(outputs, labels).item()\n",
    "            test_auc = (metric_auc(labels, preds)).item()\n",
    "            test_accuracy = correct / total\n",
    "        \n",
    "            run[\"test/batch/loss\"].append(test_loss)\n",
    "            run[\"test/batch/acc\"].append(test_accuracy)\n",
    "            run['test/batch/auc'].append(test_auc)\n",
    "            \n",
    "            accuracy.append(test_accuracy)\n",
    "            auc.append(test_auc)\n",
    "            losses.append(test_loss)\n",
    "            metric_auc.reset()\n",
    "        \n",
    "        # empty GPU RAM\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        num_epochs = params[\"num_epoch\"]\n",
    "        print(f\"epoch: {epoch+1}/{num_epochs}, Test Loss: {np.mean(losses)}, Test Acc: {np.mean(accuracy)}, Test AUC: {np.mean(auc)}\")\n",
    "        print(\"\\n\")\n",
    "        print(\"=\"*30)\n",
    "        \n",
    "        run[\"test/epoch/loss\"].append(np.mean(losses))\n",
    "        run[\"test/epoch/acc\"].append(np.mean(accuracy))\n",
    "        run['test/epoch/auc'].append(np.mean(auc))\n",
    "        \n",
    "        # scheduler\n",
    "        # scheduler.step(np.mean(auc))\n",
    "        \n",
    "        optim_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print(f\"learning rate epoch {epoch} : {optim_lr}\")\n",
    "        run[\"train/epoch/lr\"].append(optim_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303c0e71-c384-40f5-b2ed-f516c1d8bfce",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/hackthon2/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0/28 acc:0.3125, auc:0.5008909702301025, loss:1.356450461782515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/hackthon2/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1/28 acc:0.40625, auc:0.4380073547363281, loss:1.6281470830745093\n",
      "batch 2/28 acc:0.4375, auc:0.2967884838581085, loss:1.3081273986026645\n",
      "batch 3/28 acc:0.328125, auc:0.39370355010032654, loss:1.3113744878210127\n",
      "batch 4/28 acc:0.296875, auc:0.3594387248158455, loss:1.2799627119675279\n",
      "batch 5/28 acc:0.296875, auc:0.47305962443351746, loss:1.2588312681764364\n",
      "batch 6/28 acc:0.34375, auc:0.39872899651527405, loss:1.371440164744854\n",
      "batch 7/28 acc:0.421875, auc:0.4380278140306473, loss:1.2085031368769705\n",
      "batch 8/28 acc:0.4375, auc:0.4441426694393158, loss:1.3377094299066812\n",
      "batch 9/28 acc:0.40625, auc:0.46032969653606415, loss:1.3903804623732867\n",
      "batch 10/28 acc:0.515625, auc:0.4780620336532593, loss:1.1584330917103216\n",
      "batch 11/28 acc:0.484375, auc:0.4653960168361664, loss:1.1552608089405112\n",
      "batch 12/28 acc:0.484375, auc:0.4879588186740875, loss:1.2804481335915625\n",
      "batch 13/28 acc:0.40625, auc:0.4377271682024002, loss:1.480086523341015\n",
      "batch 14/28 acc:0.46875, auc:0.45827361941337585, loss:1.1877010092139244\n",
      "batch 15/28 acc:0.46875, auc:0.4597453773021698, loss:1.2475961418822408\n",
      "batch 16/28 acc:0.453125, auc:0.45028145611286163, loss:1.188932340592146\n",
      "batch 17/28 acc:0.4375, auc:0.44148746132850647, loss:1.1957037633383152\n",
      "batch 18/28 acc:0.421875, auc:0.44810639321804047, loss:1.2604233962447324\n",
      "batch 19/28 acc:0.46875, auc:0.47146567702293396, loss:1.1749140839092433\n",
      "batch 20/28 acc:0.375, auc:0.41532009840011597, loss:1.3005637023597956\n",
      "batch 21/28 acc:0.328125, auc:0.37667272239923477, loss:1.1909115896560252\n",
      "epoch: 1/25, Test Loss: 1.1988229031928561, Test Acc: 0.4283308731421462, Test AUC: 0.4324068714949218\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 0 : 0.001\n",
      "epoch 2.\n",
      "batch 0/28 acc:0.46875, auc:0.45928139984607697, loss:1.149248783942312\n",
      "batch 1/28 acc:0.390625, auc:0.4145131856203079, loss:1.3124001072719693\n",
      "batch 2/28 acc:0.578125, auc:0.5186533778905869, loss:1.1019289810210466\n",
      "batch 3/28 acc:0.53125, auc:0.4935658723115921, loss:1.2062508813105524\n",
      "batch 4/28 acc:0.421875, auc:0.44280852377414703, loss:1.3250520559959114\n",
      "batch 5/28 acc:0.546875, auc:0.5031004548072815, loss:1.094283920712769\n",
      "batch 6/28 acc:0.484375, auc:0.49311015009880066, loss:1.1397182336077094\n",
      "batch 7/28 acc:0.40625, auc:0.42424818873405457, loss:1.2281598676927388\n",
      "batch 8/28 acc:0.453125, auc:0.46408139169216156, loss:1.1039188990107505\n",
      "batch 9/28 acc:0.484375, auc:0.4539351612329483, loss:1.2690655766054988\n",
      "batch 10/28 acc:0.515625, auc:0.5177486836910248, loss:1.1194665481161792\n",
      "batch 11/28 acc:0.421875, auc:0.4305473268032074, loss:1.1417523343116045\n",
      "batch 12/28 acc:0.46875, auc:0.44449883699417114, loss:1.1866314634680748\n",
      "batch 13/28 acc:0.609375, auc:0.5340329855680466, loss:1.039763811044395\n",
      "batch 14/28 acc:0.5, auc:0.48008108139038086, loss:1.1451551892096177\n",
      "batch 15/28 acc:0.484375, auc:0.4667332321405411, loss:1.1582917322521098\n",
      "batch 16/28 acc:0.4375, auc:0.4348049759864807, loss:1.1580031481571496\n",
      "batch 17/28 acc:0.421875, auc:0.45703208446502686, loss:1.3388596277218312\n",
      "batch 18/28 acc:0.421875, auc:0.4515478312969208, loss:1.2021362069062889\n",
      "batch 19/28 acc:0.390625, auc:0.41691266000270844, loss:1.1722898540319875\n",
      "batch 20/28 acc:0.59375, auc:0.5146707594394684, loss:1.1326019074767828\n",
      "batch 21/28 acc:0.484375, auc:0.46125665307044983, loss:1.3154109348542988\n",
      "epoch: 2/25, Test Loss: 1.232729686466469, Test Acc: 0.4463492045577167, Test AUC: 0.44405053691430524\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 1 : 0.001\n",
      "epoch 3.\n",
      "batch 0/28 acc:0.5625, auc:0.5196700245141983, loss:1.0062768375501037\n",
      "batch 1/28 acc:0.46875, auc:0.47259722650051117, loss:1.1394557636231184\n",
      "batch 2/28 acc:0.453125, auc:0.4401044324040413, loss:1.179656050633639\n",
      "batch 3/28 acc:0.484375, auc:0.481614425778389, loss:1.2667919141240418\n",
      "batch 4/28 acc:0.375, auc:0.42225902527570724, loss:1.1286802769172937\n",
      "batch 5/28 acc:0.421875, auc:0.4364621043205261, loss:1.1972749482374638\n",
      "batch 6/28 acc:0.53125, auc:0.4938622862100601, loss:1.1432314056437463\n",
      "batch 7/28 acc:0.46875, auc:0.4615944176912308, loss:1.162126597482711\n",
      "batch 8/28 acc:0.609375, auc:0.545513927936554, loss:1.0116160737816244\n",
      "batch 9/28 acc:0.453125, auc:0.4705437272787094, loss:1.2158298838185146\n",
      "batch 10/28 acc:0.625, auc:0.541100263595581, loss:1.0954467760457192\n",
      "batch 11/28 acc:0.4375, auc:0.4475880265235901, loss:1.3416442691959674\n",
      "batch 12/28 acc:0.5625, auc:0.5035183131694794, loss:1.101227662875317\n",
      "batch 13/28 acc:0.578125, auc:0.5026097297668457, loss:1.107742446474731\n",
      "batch 14/28 acc:0.578125, auc:0.5233011692762375, loss:1.240050554741174\n",
      "batch 15/28 acc:0.484375, auc:0.4704368859529495, loss:1.0494912661088165\n",
      "batch 16/28 acc:0.46875, auc:0.4409768804907799, loss:1.1012849602848291\n",
      "batch 17/28 acc:0.578125, auc:0.5128878802061081, loss:0.9236953840882052\n",
      "batch 18/28 acc:0.5625, auc:0.5089464038610458, loss:1.0554814030765556\n",
      "batch 19/28 acc:0.5625, auc:0.5029845088720322, loss:1.0127591132186353\n",
      "batch 20/28 acc:0.484375, auc:0.4913930147886276, loss:1.2336941198445857\n",
      "batch 21/28 acc:0.609375, auc:0.5432291477918625, loss:1.033613979263464\n",
      "epoch: 3/25, Test Loss: 0.9805111456133256, Test Acc: 0.6251648297366704, Test AUC: 0.5449142307043076\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 2 : 0.001\n",
      "epoch 4.\n",
      "batch 0/28 acc:0.6875, auc:0.5883203744888306, loss:0.7850175263592973\n",
      "batch 1/28 acc:0.734375, auc:0.609994113445282, loss:0.8349443232873455\n",
      "batch 2/28 acc:0.6875, auc:0.5860224664211273, loss:0.7924813859863207\n",
      "batch 3/28 acc:0.765625, auc:0.6362491995096207, loss:0.8829511261428706\n",
      "batch 4/28 acc:0.796875, auc:0.640625, loss:0.642903843690874\n",
      "batch 5/28 acc:0.65625, auc:0.5668260455131531, loss:0.8387759259931045\n",
      "batch 6/28 acc:0.8125, auc:0.6492515951395035, loss:0.6223741619905923\n",
      "batch 7/28 acc:0.734375, auc:0.6026210933923721, loss:0.7506282967369771\n",
      "batch 8/28 acc:0.703125, auc:0.5854212492704391, loss:0.7022692761602229\n",
      "batch 9/28 acc:0.703125, auc:0.6006511747837067, loss:0.8164444742724299\n",
      "batch 10/28 acc:0.6875, auc:0.5774161070585251, loss:0.7659439443741576\n",
      "batch 11/28 acc:0.75, auc:0.6191974729299545, loss:0.7652360819229216\n",
      "batch 12/28 acc:0.765625, auc:0.6417643576860428, loss:0.6731953652924858\n",
      "batch 13/28 acc:0.65625, auc:0.5728002786636353, loss:0.92854231603269\n",
      "batch 14/28 acc:0.75, auc:0.6217220574617386, loss:0.7010601355577819\n",
      "batch 15/28 acc:0.703125, auc:0.5956709980964661, loss:0.8107814844406676\n",
      "batch 16/28 acc:0.71875, auc:0.616729125380516, loss:0.8065179268087377\n",
      "batch 17/28 acc:0.734375, auc:0.618511363863945, loss:0.6923170329100685\n",
      "batch 18/28 acc:0.734375, auc:0.6034664362668991, loss:0.6742781899374677\n",
      "batch 19/28 acc:0.828125, auc:0.6541066616773605, loss:0.47844249615445733\n",
      "batch 20/28 acc:0.8125, auc:0.6398023366928101, loss:0.4540304981637746\n",
      "batch 21/28 acc:0.6875, auc:0.5911382287740707, loss:0.9148281372799829\n",
      "epoch: 4/25, Test Loss: 0.8276469041604948, Test Acc: 0.6698724000120927, Test AUC: 0.5651629248803313\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 3 : 0.001\n",
      "epoch 5.\n",
      "batch 0/28 acc:0.859375, auc:0.6867741644382477, loss:0.4608284408968757\n",
      "batch 1/28 acc:0.9375, auc:0.7183270007371902, loss:0.3776805353118107\n",
      "batch 2/28 acc:0.75, auc:0.6287965327501297, loss:0.6346777570361155\n",
      "batch 3/28 acc:0.75, auc:0.6236000061035156, loss:0.5782897813551244\n",
      "batch 4/28 acc:0.65625, auc:0.5661270618438721, loss:0.7691660392083577\n",
      "batch 5/28 acc:0.75, auc:0.6211428791284561, loss:0.7501642853230805\n",
      "batch 6/28 acc:0.78125, auc:0.6375850290060043, loss:0.4858489591460966\n",
      "batch 7/28 acc:0.875, auc:0.683343693614006, loss:0.4922677495633252\n",
      "batch 8/28 acc:0.875, auc:0.6896225214004517, loss:0.5528413205656761\n",
      "batch 9/28 acc:0.875, auc:0.6857568621635437, loss:0.503761942015899\n",
      "batch 10/28 acc:0.859375, auc:0.6849283277988434, loss:0.5207771821810638\n",
      "batch 11/28 acc:0.78125, auc:0.6604967415332794, loss:0.844453391102661\n",
      "batch 12/28 acc:0.703125, auc:0.6068365424871445, loss:0.8065701136110874\n",
      "batch 13/28 acc:0.8125, auc:0.6537005007266998, loss:0.6505841666876222\n",
      "batch 14/28 acc:0.71875, auc:0.6071556210517883, loss:0.7661319564649602\n",
      "batch 15/28 acc:0.8125, auc:0.6563915610313416, loss:0.5055894548495417\n",
      "batch 16/28 acc:0.78125, auc:0.8836569786071777, loss:0.614668737136526\n",
      "batch 17/28 acc:0.78125, auc:0.6268853396177292, loss:0.5627031421317952\n",
      "batch 18/28 acc:0.8125, auc:0.6544969528913498, loss:0.5430559220549185\n",
      "batch 19/28 acc:0.84375, auc:0.662367433309555, loss:0.49324017588514835\n",
      "batch 20/28 acc:0.8125, auc:0.6562428623437881, loss:0.6247085447466816\n",
      "batch 21/28 acc:0.765625, auc:0.6225172877311707, loss:0.5895279143442167\n",
      "epoch: 5/25, Test Loss: 0.7658072897902457, Test Acc: 0.748825245741804, Test AUC: 0.60409186814319\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 4 : 0.001\n",
      "epoch 6.\n",
      "batch 0/28 acc:0.8125, auc:0.6539465934038162, loss:0.4326348148024408\n",
      "batch 1/28 acc:0.75, auc:0.7454571723937988, loss:0.6106643460225314\n",
      "batch 2/28 acc:0.8125, auc:0.6547662317752838, loss:0.485708456755674\n",
      "batch 3/28 acc:0.875, auc:0.6739652007818222, loss:0.39544539809867274\n",
      "batch 4/28 acc:0.796875, auc:0.6386591494083405, loss:0.5367902485522791\n",
      "batch 5/28 acc:0.84375, auc:0.6716683506965637, loss:0.42030548291950254\n",
      "batch 6/28 acc:0.796875, auc:0.6471703350543976, loss:0.5585054587718332\n",
      "batch 7/28 acc:0.828125, auc:0.6594865173101425, loss:0.44143391947000055\n",
      "batch 8/28 acc:0.796875, auc:0.6514560580253601, loss:0.49347324154223315\n",
      "batch 9/28 acc:0.765625, auc:0.631061002612114, loss:0.6205126411223318\n",
      "batch 10/28 acc:0.78125, auc:0.6419110745191574, loss:0.4597700986851123\n",
      "batch 11/28 acc:0.84375, auc:0.6711935698986053, loss:0.40230390690703643\n",
      "batch 12/28 acc:0.84375, auc:0.6740473359823227, loss:0.46355567507634987\n",
      "batch 13/28 acc:0.84375, auc:0.6770664900541306, loss:0.6945943885530141\n",
      "batch 14/28 acc:0.875, auc:0.6894738227128983, loss:0.45661638791352743\n",
      "batch 15/28 acc:0.8125, auc:0.6734072268009186, loss:0.5672918482669047\n",
      "batch 16/28 acc:0.78125, auc:0.6363655477762222, loss:0.5350582551400294\n",
      "batch 17/28 acc:0.84375, auc:0.7863797545433044, loss:0.542827041077544\n",
      "batch 18/28 acc:0.859375, auc:0.6725805252790451, loss:0.434548657911364\n",
      "batch 19/28 acc:0.84375, auc:0.6774262636899948, loss:0.535119252926961\n",
      "batch 20/28 acc:0.796875, auc:0.6557727605104446, loss:0.5874509675122681\n",
      "batch 21/28 acc:0.8125, auc:0.6492856442928314, loss:0.4774363275500946\n",
      "epoch: 6/25, Test Loss: 0.872118744251731, Test Acc: 0.7067299412037795, Test AUC: 0.5977633168751543\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 5 : 0.001\n",
      "epoch 7.\n",
      "batch 0/28 acc:0.796875, auc:0.887650728225708, loss:0.5675464005616959\n",
      "batch 1/28 acc:0.90625, auc:0.7043286114931107, loss:0.30891130211239215\n",
      "batch 2/28 acc:0.953125, auc:0.7248577326536179, loss:0.26422489479591604\n",
      "batch 3/28 acc:0.953125, auc:0.7320323288440704, loss:0.23805723639088683\n",
      "batch 4/28 acc:0.84375, auc:0.6651347875595093, loss:0.46527423716906924\n",
      "batch 5/28 acc:0.921875, auc:0.7141369134187698, loss:0.2611707166506676\n",
      "batch 6/28 acc:0.921875, auc:0.9578894376754761, loss:0.24427843114972347\n",
      "batch 7/28 acc:0.9375, auc:0.7147961407899857, loss:0.25415465103287715\n",
      "batch 8/28 acc:0.859375, auc:0.6747010201215744, loss:0.235879534506239\n",
      "batch 9/28 acc:0.890625, auc:0.6920140534639359, loss:0.35754658662335714\n",
      "batch 10/28 acc:0.921875, auc:0.715302363038063, loss:0.26446936716820346\n",
      "batch 11/28 acc:0.9375, auc:0.7155182808637619, loss:0.21919188200627104\n",
      "batch 12/28 acc:0.875, auc:0.8146872520446777, loss:0.3004397911372507\n",
      "batch 13/28 acc:0.90625, auc:0.717349573969841, loss:0.31261547937265277\n",
      "batch 14/28 acc:0.9375, auc:0.7211174517869949, loss:0.3715834205236206\n",
      "batch 15/28 acc:0.890625, auc:0.696454718708992, loss:0.24647428671642047\n",
      "batch 16/28 acc:0.90625, auc:0.7037203907966614, loss:0.23095699804753167\n",
      "batch 17/28 acc:0.921875, auc:0.9537463784217834, loss:0.279477740406719\n",
      "batch 18/28 acc:0.859375, auc:0.8093459606170654, loss:0.5140389483949548\n",
      "batch 19/28 acc:0.875, auc:0.692518949508667, loss:0.3333394006567687\n",
      "batch 20/28 acc:0.828125, auc:0.90428626537323, loss:0.5913394414515096\n",
      "batch 21/28 acc:0.875, auc:0.9337419271469116, loss:0.4119199372580624\n",
      "epoch: 7/25, Test Loss: 1.0024110850292594, Test Acc: 0.7035953679372313, Test AUC: 0.6852090507745743\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 6 : 0.001\n",
      "epoch 8.\n",
      "batch 0/28 acc:0.90625, auc:0.948971152305603, loss:0.3166454156776126\n",
      "batch 1/28 acc:0.90625, auc:0.9458626508712769, loss:0.23556648185763152\n",
      "batch 2/28 acc:0.859375, auc:0.8386464715003967, loss:0.2948108952380153\n",
      "batch 3/28 acc:0.921875, auc:0.9577928781509399, loss:0.3499378261731181\n",
      "batch 4/28 acc:0.875, auc:0.9258047342300415, loss:0.4659253970530699\n",
      "batch 5/28 acc:0.9375, auc:0.9641131162643433, loss:0.2264571606438608\n",
      "batch 6/28 acc:0.9375, auc:0.8436636924743652, loss:0.1987730683663358\n",
      "batch 7/28 acc:0.828125, auc:0.6701430827379227, loss:0.42791344636134454\n",
      "batch 8/28 acc:0.890625, auc:0.9472365379333496, loss:0.2554059507092461\n",
      "batch 9/28 acc:0.875, auc:0.936246931552887, loss:0.3341804791480172\n",
      "batch 10/28 acc:0.96875, auc:0.9835885167121887, loss:0.12367344811354997\n",
      "batch 11/28 acc:0.90625, auc:0.8295188546180725, loss:0.1991946889411338\n",
      "batch 12/28 acc:0.90625, auc:0.8390063047409058, loss:0.18856289083123556\n",
      "batch 13/28 acc:0.9375, auc:0.9659525156021118, loss:0.21509759098262293\n",
      "batch 14/28 acc:0.84375, auc:0.9158803224563599, loss:0.4360827911496017\n",
      "batch 15/28 acc:0.9375, auc:0.9645043015480042, loss:0.22683092780243896\n",
      "batch 16/28 acc:0.890625, auc:0.8812360167503357, loss:0.2449777446754524\n",
      "batch 17/28 acc:0.875, auc:0.934802770614624, loss:0.30313081374697504\n",
      "batch 18/28 acc:0.90625, auc:0.7039367407560349, loss:0.37198879295965526\n",
      "batch 19/28 acc:0.90625, auc:0.9477381110191345, loss:0.3927365907784406\n",
      "batch 20/28 acc:0.859375, auc:0.8659337759017944, loss:0.41393256558512803\n",
      "batch 21/28 acc:0.890625, auc:0.8241486549377441, loss:0.32986371813058213\n",
      "epoch: 8/25, Test Loss: 0.9749491435059926, Test Acc: 0.7338343354867682, Test AUC: 0.6533648784865033\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 7 : 0.001\n",
      "epoch 9.\n",
      "batch 0/28 acc:0.859375, auc:0.8615878820419312, loss:0.42359210713175344\n",
      "batch 1/28 acc:0.921875, auc:0.7164368629455566, loss:0.3549555010868062\n",
      "batch 2/28 acc:0.859375, auc:0.8664547204971313, loss:0.3800270752617507\n",
      "batch 3/28 acc:0.890625, auc:0.8218079805374146, loss:0.32777816037560115\n",
      "batch 4/28 acc:0.84375, auc:0.7958424687385559, loss:0.33032831811397045\n",
      "batch 5/28 acc:0.90625, auc:0.703142061829567, loss:0.23494745395873906\n",
      "batch 6/28 acc:0.921875, auc:0.9566234946250916, loss:0.21333238574879942\n",
      "batch 7/28 acc:0.96875, auc:0.7355640679597855, loss:0.17070848700677743\n",
      "batch 8/28 acc:0.921875, auc:0.7117404490709305, loss:0.1908279447179666\n",
      "batch 9/28 acc:0.9375, auc:0.7208874672651291, loss:0.22988745619113615\n",
      "batch 10/28 acc:0.875, auc:0.6862554103136063, loss:0.49593947298490093\n",
      "batch 11/28 acc:0.921875, auc:0.8394950032234192, loss:0.34491767905274173\n",
      "batch 12/28 acc:0.953125, auc:0.9389979839324951, loss:0.1932286187857244\n",
      "batch 13/28 acc:0.875, auc:0.9299633502960205, loss:0.44424163426629093\n",
      "batch 14/28 acc:0.90625, auc:0.8709269762039185, loss:0.25792880857989076\n",
      "batch 15/28 acc:0.84375, auc:0.9138796925544739, loss:0.6026001464852015\n",
      "batch 16/28 acc:0.828125, auc:0.7907938957214355, loss:0.4521553524573392\n",
      "batch 17/28 acc:0.953125, auc:0.8558053970336914, loss:0.2857734970020829\n",
      "batch 18/28 acc:0.859375, auc:0.9134762287139893, loss:0.4832643739296145\n",
      "batch 19/28 acc:0.90625, auc:0.9471262097358704, loss:0.30710084141537664\n",
      "batch 20/28 acc:0.921875, auc:0.959898829460144, loss:0.22691106131060224\n",
      "batch 21/28 acc:0.953125, auc:0.7280844151973724, loss:0.2387548181168313\n",
      "epoch: 9/25, Test Loss: 0.9123686903365276, Test Acc: 0.7235349618922919, Test AUC: 0.6568552062592723\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 8 : 0.001\n",
      "epoch 10.\n",
      "batch 0/28 acc:0.921875, auc:0.9574267864227295, loss:0.26554580333777267\n",
      "batch 1/28 acc:0.953125, auc:0.9736860990524292, loss:0.2764414282537473\n",
      "batch 2/28 acc:0.953125, auc:0.9477272629737854, loss:0.17559633105838657\n",
      "batch 3/28 acc:0.96875, auc:0.9272787570953369, loss:0.15136815706136986\n",
      "batch 4/28 acc:0.921875, auc:0.9638293981552124, loss:0.20795306133095437\n",
      "batch 5/28 acc:0.953125, auc:0.7295266687870026, loss:0.18882662637543035\n",
      "batch 6/28 acc:0.984375, auc:0.9914048910140991, loss:0.08338011776277199\n",
      "batch 7/28 acc:0.953125, auc:0.9379057884216309, loss:0.17273453663210603\n",
      "batch 8/28 acc:0.890625, auc:0.9434118270874023, loss:0.36323300875938\n",
      "batch 9/28 acc:0.96875, auc:0.9836231470108032, loss:0.1358130877210897\n",
      "batch 10/28 acc:0.9375, auc:0.9064563512802124, loss:0.2192171645911003\n",
      "batch 11/28 acc:0.953125, auc:0.974945068359375, loss:0.10388170643318517\n",
      "batch 12/28 acc:0.96875, auc:0.9800205230712891, loss:0.10726818079001532\n",
      "batch 13/28 acc:0.953125, auc:0.9774962663650513, loss:0.12064925090908218\n",
      "batch 14/28 acc:0.921875, auc:0.946218729019165, loss:0.3598101844231678\n",
      "batch 15/28 acc:0.984375, auc:0.9910063743591309, loss:0.08629964521014699\n",
      "batch 16/28 acc:0.953125, auc:0.734375, loss:0.15189530482439295\n",
      "batch 17/28 acc:0.921875, auc:0.8400473594665527, loss:0.13794240767492738\n",
      "batch 18/28 acc:1.0, auc:1.0, loss:0.05532722129191825\n",
      "batch 19/28 acc:0.90625, auc:0.9480675458908081, loss:0.2788938466030686\n",
      "batch 20/28 acc:0.9375, auc:0.705038771033287, loss:0.1991448398139255\n",
      "batch 21/28 acc:0.96875, auc:0.9830709099769592, loss:0.11279437746634358\n",
      "epoch: 10/25, Test Loss: 1.0827412557805887, Test Acc: 0.7465212978085023, Test AUC: 0.6983104388822209\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 9 : 0.001\n",
      "epoch 11.\n",
      "batch 0/28 acc:0.984375, auc:0.992775559425354, loss:0.052014998826962255\n",
      "batch 1/28 acc:0.9375, auc:0.9125473499298096, loss:0.11534799666878826\n",
      "batch 2/28 acc:0.96875, auc:0.9851416349411011, loss:0.09445199743095145\n",
      "batch 3/28 acc:0.953125, auc:0.973710298538208, loss:0.17145531290657345\n",
      "batch 4/28 acc:0.96875, auc:0.9738428592681885, loss:0.10770681767544943\n",
      "batch 5/28 acc:1.0, auc:1.0, loss:0.05022054256244246\n",
      "batch 6/28 acc:0.921875, auc:0.9342600703239441, loss:0.16109980753344644\n",
      "batch 7/28 acc:0.953125, auc:0.9522961378097534, loss:0.2664377927337682\n",
      "batch 8/28 acc:0.96875, auc:0.9267113208770752, loss:0.1262785253469474\n",
      "batch 9/28 acc:0.90625, auc:0.9149227738380432, loss:0.21470047314983276\n",
      "batch 10/28 acc:0.984375, auc:0.9935516119003296, loss:0.09915908686662078\n",
      "batch 11/28 acc:0.96875, auc:0.97966468334198, loss:0.19357571327611822\n",
      "batch 12/28 acc:0.90625, auc:0.944745659828186, loss:0.24409501875561546\n",
      "batch 13/28 acc:0.90625, auc:0.9430363774299622, loss:0.41519421022212555\n",
      "batch 14/28 acc:0.953125, auc:0.9375209808349609, loss:0.20461873987858326\n",
      "batch 15/28 acc:0.890625, auc:0.9415699243545532, loss:0.3503492630017604\n",
      "batch 16/28 acc:0.921875, auc:0.8385046720504761, loss:0.18927209476680673\n",
      "batch 17/28 acc:0.9375, auc:0.8457499742507935, loss:0.13408940377462386\n",
      "batch 18/28 acc:0.984375, auc:0.992775559425354, loss:0.0880808034176539\n",
      "batch 19/28 acc:0.9375, auc:0.9634860754013062, loss:0.14669802797126508\n",
      "batch 20/28 acc:0.984375, auc:0.9923020601272583, loss:0.059640443697190904\n",
      "batch 21/28 acc:0.96875, auc:0.981220006942749, loss:0.1304377766542757\n",
      "epoch: 11/25, Test Loss: 1.1303334838277879, Test Acc: 0.7408393965689298, Test AUC: 0.6387988593090664\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 10 : 0.001\n",
      "epoch 12.\n",
      "batch 0/28 acc:0.96875, auc:0.9829690456390381, loss:0.08666473177365219\n",
      "batch 1/28 acc:0.984375, auc:0.9911931753158569, loss:0.07111725620998754\n",
      "batch 2/28 acc:0.890625, auc:0.8116567134857178, loss:0.2562028252728794\n",
      "batch 3/28 acc:0.96875, auc:0.9806227684020996, loss:0.18033481179767819\n",
      "batch 4/28 acc:0.984375, auc:0.9909722805023193, loss:0.10084525400748134\n",
      "batch 5/28 acc:0.96875, auc:0.9794849157333374, loss:0.16264240665287844\n",
      "batch 6/28 acc:0.953125, auc:0.8557624816894531, loss:0.09757531388322604\n",
      "batch 7/28 acc:0.921875, auc:0.9242825508117676, loss:0.12913440946931587\n",
      "batch 8/28 acc:0.9375, auc:0.8504650592803955, loss:0.16651143101739763\n",
      "batch 9/28 acc:0.96875, auc:0.9846041202545166, loss:0.1398042637850807\n",
      "batch 10/28 acc:0.921875, auc:0.9293476939201355, loss:0.18514486090299442\n",
      "batch 11/28 acc:1.0, auc:1.0, loss:0.03866945508112707\n",
      "batch 12/28 acc:0.9375, auc:0.9648357629776001, loss:0.186836250018942\n",
      "batch 13/28 acc:0.96875, auc:0.926175594329834, loss:0.09952232540411998\n",
      "batch 14/28 acc:0.953125, auc:0.9679946899414062, loss:0.10256494308498532\n",
      "batch 15/28 acc:0.890625, auc:0.9407781362533569, loss:0.2313628792929876\n",
      "batch 16/28 acc:0.9375, auc:0.9408730268478394, loss:0.18748605752999126\n",
      "batch 17/28 acc:0.953125, auc:0.8538799285888672, loss:0.2100719752344986\n",
      "batch 18/28 acc:0.921875, auc:0.900377631187439, loss:0.2098283215031529\n",
      "batch 19/28 acc:0.96875, auc:0.9825438261032104, loss:0.10123403672143638\n",
      "batch 20/28 acc:0.953125, auc:0.9744399785995483, loss:0.1693405983482208\n",
      "batch 21/28 acc:0.921875, auc:0.958835244178772, loss:0.20902108285679333\n",
      "epoch: 12/25, Test Loss: 1.169494854292868, Test Acc: 0.7302822228833258, Test AUC: 0.6626067412170497\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 11 : 0.001\n",
      "epoch 13.\n",
      "batch 0/28 acc:0.96875, auc:0.9804446697235107, loss:0.09124648311779993\n",
      "batch 1/28 acc:0.96875, auc:0.9249680042266846, loss:0.11222180639674662\n",
      "batch 2/28 acc:0.984375, auc:0.9919642806053162, loss:0.07339149171320969\n",
      "batch 3/28 acc:0.96875, auc:0.9549817442893982, loss:0.05535111207478849\n",
      "batch 4/28 acc:1.0, auc:1.0, loss:0.053111861747794364\n",
      "batch 5/28 acc:0.984375, auc:0.9918478727340698, loss:0.081439388328306\n",
      "batch 6/28 acc:0.96875, auc:0.9112318754196167, loss:0.05529243106494164\n",
      "batch 7/28 acc:0.953125, auc:0.973135232925415, loss:0.16336920354547146\n",
      "batch 8/28 acc:1.0, auc:1.0, loss:0.02144642946490194\n",
      "batch 9/28 acc:0.953125, auc:0.977745771408081, loss:0.09466370948675262\n",
      "batch 10/28 acc:0.96875, auc:0.9819232225418091, loss:0.13793041190751865\n",
      "batch 11/28 acc:0.9375, auc:0.9615715146064758, loss:0.27909416479292304\n",
      "batch 12/28 acc:0.984375, auc:0.9910904169082642, loss:0.05909832489905398\n",
      "batch 13/28 acc:0.96875, auc:0.9250378608703613, loss:0.0781746933055274\n",
      "batch 14/28 acc:0.921875, auc:0.9540637731552124, loss:0.11762230393620143\n",
      "batch 15/28 acc:0.953125, auc:0.916580855846405, loss:0.20220344302516224\n",
      "batch 16/28 acc:0.9375, auc:0.8474477529525757, loss:0.13770660922654088\n",
      "batch 17/28 acc:0.921875, auc:0.9029905796051025, loss:0.29617323682333563\n",
      "batch 18/28 acc:0.96875, auc:0.9835460186004639, loss:0.20058488374047556\n",
      "batch 19/28 acc:0.96875, auc:0.9571157693862915, loss:0.052268261087732526\n",
      "batch 20/28 acc:0.984375, auc:0.9922348260879517, loss:0.06375019476706711\n",
      "batch 21/28 acc:0.96875, auc:0.9838328957557678, loss:0.10283517206826787\n",
      "epoch: 13/25, Test Loss: 1.3315488005199863, Test Acc: 0.6997895973057809, Test AUC: 0.6461470398035917\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 12 : 0.001\n",
      "epoch 14.\n",
      "batch 0/28 acc:1.0, auc:1.0, loss:0.029788156408073974\n",
      "batch 1/28 acc:0.96875, auc:0.9809126853942871, loss:0.08385329092192251\n",
      "batch 2/28 acc:0.953125, auc:0.9754043221473694, loss:0.17488925384589038\n",
      "batch 3/28 acc:0.953125, auc:0.9659262299537659, loss:0.1829841361079616\n",
      "batch 4/28 acc:0.984375, auc:0.9917874336242676, loss:0.05925334970606855\n",
      "batch 5/28 acc:0.984375, auc:0.991032600402832, loss:0.04194257926798173\n",
      "batch 6/28 acc:1.0, auc:1.0, loss:0.05411675896264967\n",
      "batch 7/28 acc:1.0, auc:1.0, loss:0.0504551338589323\n",
      "batch 8/28 acc:0.953125, auc:0.9730796813964844, loss:0.08586914708862992\n",
      "batch 9/28 acc:1.0, auc:1.0, loss:0.0600890172209958\n",
      "batch 10/28 acc:0.96875, auc:0.9831292629241943, loss:0.12368470064123471\n",
      "batch 11/28 acc:0.953125, auc:0.9378697872161865, loss:0.10179444912500912\n",
      "batch 12/28 acc:0.96875, auc:0.9835407733917236, loss:0.07836671664822603\n",
      "batch 13/28 acc:0.96875, auc:0.9794892072677612, loss:0.1460720434153835\n",
      "batch 14/28 acc:0.984375, auc:0.9891387224197388, loss:0.10751424843228108\n",
      "batch 15/28 acc:1.0, auc:1.0, loss:0.028290018698612585\n",
      "batch 16/28 acc:0.9375, auc:0.9703243970870972, loss:0.17824988600983716\n",
      "batch 17/28 acc:0.9375, auc:0.9621855020523071, loss:0.20594619489759225\n",
      "batch 18/28 acc:0.953125, auc:0.7321590930223465, loss:0.12019780044968087\n",
      "batch 19/28 acc:0.9375, auc:0.9253472089767456, loss:0.2085699924154767\n",
      "batch 20/28 acc:0.921875, auc:0.8394708633422852, loss:0.28935945133821406\n",
      "batch 21/28 acc:0.96875, auc:0.9434211254119873, loss:0.09940093084483337\n",
      "epoch: 14/25, Test Loss: 1.2441264489278006, Test Acc: 0.7198095285503814, Test AUC: 0.7222803885286505\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 13 : 0.001\n",
      "epoch 15.\n",
      "batch 0/28 acc:0.9375, auc:0.9357056617736816, loss:0.2036512471592573\n",
      "batch 1/28 acc:0.9375, auc:0.9255814552307129, loss:0.1761499652300671\n",
      "batch 2/28 acc:0.953125, auc:0.9732238054275513, loss:0.11769009391393581\n",
      "batch 3/28 acc:0.96875, auc:0.9833327531814575, loss:0.08897958206503631\n",
      "batch 4/28 acc:0.953125, auc:0.9333333969116211, loss:0.14178097553379132\n",
      "batch 5/28 acc:0.953125, auc:0.9747331738471985, loss:0.1372439171339579\n",
      "batch 6/28 acc:0.96875, auc:0.9822895526885986, loss:0.05477817991581446\n",
      "batch 7/28 acc:0.9375, auc:0.9616202712059021, loss:0.20709723667578217\n",
      "batch 8/28 acc:0.984375, auc:0.9902146458625793, loss:0.03884946401151268\n",
      "batch 9/28 acc:0.984375, auc:0.9905637502670288, loss:0.0697065791692344\n",
      "batch 10/28 acc:0.953125, auc:0.9551857709884644, loss:0.1584033139434844\n",
      "batch 11/28 acc:0.890625, auc:0.8672614097595215, loss:0.21788123707925244\n",
      "batch 12/28 acc:0.96875, auc:0.9839348793029785, loss:0.09500068892544888\n",
      "batch 13/28 acc:0.96875, auc:0.9833172559738159, loss:0.08746485135629456\n"
     ]
    }
   ],
   "source": [
    "train(model, params, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd7dba6-2f99-406c-93f1-08ef8300d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65580d05-4446-4625-bf5d-39f12584621a",
   "metadata": {},
   "source": [
    "### Save Model State "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a47819-ec7f-4f75-aba5-84f515e70730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37905a83-2063-4b0e-8601-d0d82e338a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(efficientB0.state_dict(), 'model_weight_efficientB0_230513(1).pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2042f8d8-4612-4cb8-abd7-f0ffd8f01b07",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc00b362-b9ff-4502-870a-cf075ca6ea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "pred_list.append(['image_id', 'healthy', 'multiple_diseases', 'rust', 'scab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbe0d55-c976-45dc-b035-e0078ff1c2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data = pd.read_csv('test.csv')\n",
    "submission_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58118079-5d5f-4381-997e-69884ad2e491",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_path = '/home/dmsai2/Desktop/AI-Study/PlantPathology2020/images/Submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144ce262-2b4d-4b3c-aa10-ed9c2a1cd200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0b4566-34f9-4612-aeda-0ab132a4a5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(submission_data))):\n",
    "    img_title = submission_data.iloc[i, 0]\n",
    "    img_dir = img_title + '.jpg'\n",
    "    # print(op(submission_path, img_dir))\n",
    "    \n",
    "    image = read_image(op(submission_path, img_dir))\n",
    "    image = image.type(torch.FloatTensor)\n",
    "    img = image / 255.\n",
    "    img = tf(img)\n",
    "    img = img.reshape(1, 3, 224, 224)\n",
    "    inputs = img.to(device)\n",
    "    # print(inputs.shape)\n",
    "    outputs = res50(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    ans = [img_title, 0, 0, 0, 0]\n",
    "    ans[int(preds) + 1] = 1\n",
    "    pred_list.append(ans)\n",
    "print(len(pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1899fd-d909-4f6f-856b-bbad34ec5538",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(pred_list)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5995e9-b1b5-46bc-8666-cdb00d001816",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('./submission/submission_efnet1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3aaec6-e5b2-4371-811b-9ec7ef083edb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
