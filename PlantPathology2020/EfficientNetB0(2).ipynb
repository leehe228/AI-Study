{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c9daf2a-dfa6-4ea1-9e44-1b15f962814a",
   "metadata": {},
   "source": [
    "# Plant Pathology 2020 - EfficientNetB0\n",
    "https://www.kaggle.com/c/plant-pathology-2020-fgvc7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a8c285-c0aa-459f-ae45-7c5b137d4120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "op = os.path.join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8839e662-2853-471b-aa65-a25ea44cac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0263a678-0a28-4260-9c55-6498f78aacd8",
   "metadata": {},
   "source": [
    "### CUDA GPU Device Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67b37a6f-2f14-42b1-896e-ba70f7908836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17bc27b-4d82-4e8d-8e51-f8eb610aee6b",
   "metadata": {},
   "source": [
    "### Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "661fe7f3-bde5-4a2a-bc90-d3f2a679a527",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/dmsai2/Desktop/AI-Study/PlantPathology2020/\"\n",
    "train_csv = op(path, \"train.csv\")\n",
    "train_path = op(path, \"images\", \"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7cd4b64-ce56-48f7-be85-d1a65e318b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1821\n"
     ]
    }
   ],
   "source": [
    "print(\"train:\", len(os.listdir(train_path)))\n",
    "n_train_data = len(os.listdir(train_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32891f11-618b-451a-a089-d506a38aac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e6cced-7eaa-41e7-91a2-d379c8b68de3",
   "metadata": {},
   "source": [
    "### Transform for DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "873caa9a-4791-4031-b9ad-860026e852c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f4efb52-36ec-44ba-96b1-e3b691823239",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    # transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563f9b7b-d83b-40e3-a5c3-f99ae1a60610",
   "metadata": {},
   "source": [
    "### PyTorch Customized `Datasets` Class\n",
    "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39f8dd8d-6084-4335-a4ae-40703fb1f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2fcebce-13ee-4a14-9bec-faded67e790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantPathologyDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None, header=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path + '.jpg')\n",
    "        image = image.type(torch.FloatTensor)\n",
    "        image = image / 255.\n",
    "    \n",
    "        label = np.argmax(self.img_labels.iloc[idx, 1:].values)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b8f3a3-072d-43a5-af2a-8262c1846b8e",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49bd803c-70b4-4e2f-a31d-385bdb8ccc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PlantPathologyDataset(train_csv, train_path, transform=tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a272d77-ea17-4411-b4bc-b2c3c825f159",
   "metadata": {},
   "source": [
    "### Show Image Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f60ffbd2-8908-4eae-93a2-6472283b529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(data, is_test=False):\n",
    "    f, ax = plt.subplots(5, 5, figsize=(15, 10))\n",
    "    \n",
    "    for i in range(25):\n",
    "        img_dir = data.img_labels.iloc[i, 0]\n",
    "        img_data = cv2.imread(op(train_path, img_dir + '.jpg'))\n",
    "        label = np.argmax(data.img_labels.iloc[0, 1:].values)\n",
    "        \n",
    "        if label  == 0:  str_label = 'healthy'\n",
    "        elif label == 1:  str_label = 'multiple_diseases'\n",
    "        elif label == 2: str_label = 'rust'\n",
    "        else: str_label = 'scab'\n",
    "        if(is_test): str_label=\"None\"\n",
    "        \n",
    "        ax[i//5, i%5].imshow(img_data)\n",
    "        ax[i//5, i%5].axis('off')\n",
    "        ax[i//5, i%5].set_title(\"Label: {}\".format(str_label))\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "515b521c-c41f-48fa-b57d-dab26230a15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_images(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7610afc-8d86-4348-a0bd-c1d559d76be0",
   "metadata": {},
   "source": [
    "### Train Test Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70b89da6-72ae-4e03-850c-67d62864ca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * 0.8)\n",
    "test_size = dataset_size - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a89d152-4d7d-46c7-84a0-58debcbd0b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73965efa-ad2c-48a1-894c-aca1fdbdb0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size : 1456\n",
      "Testing Data Size : 365\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Data Size : {len(train_dataset)}\")\n",
    "print(f\"Testing Data Size : {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edffddc-a696-4997-8cd6-0dba37a7125c",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ecbb1c7-f0ad-42c7-95fb-b57d81f3d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f5987f6-be47-404e-aa79-d9aaca4a75bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              shuffle=True, \n",
    "                              drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             batch_size=TEST_BATCH_SIZE, \n",
    "                             shuffle=True, \n",
    "                             drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "656ab8b0-0503-4650-b523-9c675f4284e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/hackthon2/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 3, 224, 224])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {X_train.size()}\")\n",
    "print(f\"Labels batch shape: {y_train.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d84243-a8b7-4b88-a389-4d9714484c35",
   "metadata": {},
   "source": [
    "### Activation Function (Swish based ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c29565a0-e61a-4819-b95e-25f51b1c0cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_fn(x):\n",
    "    \"\"\" Swish activation function \"\"\"\n",
    "    return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda0ffaa-8d90-4f23-9cd8-3adf1aa547d0",
   "metadata": {},
   "source": [
    "### EfficientNetB0 Model\n",
    "https://startnow95.tistory.com/4 <br>\n",
    "https://deep-learning-study.tistory.com/563"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31edd4d5-7d2d-424a-8875-f3e8636d4696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fb0211-4ac8-4b01-b682-edeebdad7467",
   "metadata": {},
   "source": [
    "class Conv2dSamePadding(nn.Conv2d):\n",
    "    \"\"\" 2D Convolutions like TensorFlow \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ih, iw = x.size()[-2:]\n",
    "        kh, kw = self.weight.size()[-2:]\n",
    "        sh, sw = self.stride\n",
    "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
    "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
    "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            x = F.pad(x, [pad_w//2, pad_w - pad_w//2, pad_h//2, pad_h - pad_h//2])\n",
    "        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2205329c-8672-44b3-b96d-5ebfe8534386",
   "metadata": {},
   "source": [
    "def drop_connect(inputs, p, training):\n",
    "    \"\"\" Drop connect. \"\"\"\n",
    "    if not training: return inputs\n",
    "    batch_size = inputs.shape[0]\n",
    "    keep_prob = 1 - p\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype)  # uniform [0,1)\n",
    "    binary_tensor = torch.floor(random_tensor)\n",
    "    output = inputs / keep_prob * binary_tensor\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676f4e4a-0b74-475a-8510-1403f04cfc32",
   "metadata": {},
   "source": [
    "class MBConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Mobile Inverted Residual Bottleneck Block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size, stride, expand_ratio, input_filters, output_filters, se_ratio, drop_n_add):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._bn_mom = 0.1\n",
    "        self._bn_eps = 1e-03\n",
    "        self.has_se = (se_ratio is not None) and (0 < se_ratio <= 1)\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.drop_n_add = drop_n_add\n",
    "\n",
    "        # Filter Expansion phase\n",
    "        inp = input_filters  # number of input channels\n",
    "        oup = input_filters * expand_ratio  # number of output channels\n",
    "        if expand_ratio != 1: # add it except at first block \n",
    "            self._expand_conv = Conv2dSamePadding(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n",
    "            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "\n",
    "        # Depthwise convolution phase\n",
    "        k = kernel_size\n",
    "        s = stride\n",
    "        self._depthwise_conv = Conv2dSamePadding(\n",
    "            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise(conv filter by filter)\n",
    "            kernel_size=k, stride=s, bias=False)\n",
    "        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "\n",
    "        # Squeeze and Excitation layer, if desired\n",
    "        if self.has_se:\n",
    "            num_squeezed_channels = max(1,int(input_filters * se_ratio))  # input channel * 0.25 ex) block2 => 16 * 0.25 = 4\n",
    "            self._se_reduce = Conv2dSamePadding(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n",
    "            self._se_expand = Conv2dSamePadding(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n",
    "\n",
    "        # Output phase\n",
    "        final_oup = output_filters\n",
    "        self._project_conv = Conv2dSamePadding(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n",
    "        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "        \n",
    "    def forward(self, inputs, drop_connect_rate=0.2):\n",
    "\n",
    "        # Expansion and Depthwise Convolution\n",
    "        x = inputs.to(device)\n",
    "        if self.expand_ratio != 1:\n",
    "            x = relu_fn(self._bn0(self._expand_conv(inputs)))\n",
    "        x = relu_fn(self._bn1(self._depthwise_conv(x)))\n",
    "\n",
    "        # Squeeze and Excitation\n",
    "        if self.has_se:\n",
    "            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n",
    "            x_squeezed = self._se_expand(relu_fn(self._se_reduce(x_squeezed)))\n",
    "            x = torch.sigmoid(x_squeezed) * x\n",
    "            \n",
    "        # Output phase\n",
    "        x = self._bn2(self._project_conv(x))\n",
    "\n",
    "        # Skip connection and drop connect\n",
    "        if self.drop_n_add == True:\n",
    "            if drop_connect_rate:\n",
    "                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n",
    "            x = x + inputs  # skip connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bfeb67-bceb-4e33-a0dc-332910135402",
   "metadata": {},
   "source": [
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Batch norm parameters\n",
    "        bn_mom = 0.1\n",
    "        bn_eps = 1e-03\n",
    "\n",
    "        # stem\n",
    "        in_channels = 3\n",
    "        out_channels = 32\n",
    "        self._conv_stem = Conv2dSamePadding(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n",
    "        self._bn0 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
    "\n",
    "        # Build blocks\n",
    "        self._blocks = nn.ModuleList([]) # list 형태로 model 구성할 때\n",
    "        # stage2 r1_k3_s11_e1_i32_o16_se0.25\n",
    "        self._blocks.append(MBConvBlock(kernel_size=3, stride=1, expand_ratio=1, input_filters=32, output_filters=16, se_ratio=0.25, drop_n_add=False))\n",
    "        # stage3 r2_k3_s22_e6_i16_o24_se0.25\n",
    "        self._blocks.append(MBConvBlock(3, 2, 6, 16, 24, 0.25, False))\n",
    "        self._blocks.append(MBConvBlock(3, 1, 6, 24, 24, 0.25, True))\n",
    "        # stage4 r2_k5_s22_e6_i24_o40_se0.25\n",
    "        self._blocks.append(MBConvBlock(5, 2, 6, 24, 40, 0.25, False))\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 40, 40, 0.25, True))\n",
    "        # stage5 r3_k3_s22_e6_i40_o80_se0.25\n",
    "        self._blocks.append(MBConvBlock(3, 2, 6, 40, 80, 0.25, False))\n",
    "        self._blocks.append(MBConvBlock(3, 1, 6, 80, 80, 0.25, True))\n",
    "        self._blocks.append(MBConvBlock(3, 1, 6, 80, 80, 0.25, True))\n",
    "        # stage6 r3_k5_s11_e6_i80_o112_se0.25\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 80,  112, 0.25, False))\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 112, 112, 0.25, True))\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 112, 112, 0.25, True))\n",
    "        # stage7 r4_k5_s22_e6_i112_o192_se0.25\n",
    "        self._blocks.append(MBConvBlock(5, 2, 6, 112, 192, 0.25, False))\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 192, 192, 0.25, True))\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 192, 192, 0.25, True))\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 192, 192, 0.25, True))\n",
    "        # stage8 r1_k3_s11_e6_i192_o320_se0.25\n",
    "        self._blocks.append(MBConvBlock(3, 1, 6, 192, 320, 0.25, False))\n",
    "\n",
    "        # Head \n",
    "        in_channels = 320\n",
    "        out_channels = 1280\n",
    "        self._conv_head = Conv2dSamePadding(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self._bn1 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
    "\n",
    "        # Final linear layer\n",
    "        self._dropout = 0.2\n",
    "        self._num_classes = 10\n",
    "        self._fc = nn.Linear(out_channels, self._num_classes)\n",
    "\n",
    "    def extract_features(self, inputs):\n",
    "        \"\"\" Returns output of the final convolution layer \"\"\"\n",
    "\n",
    "        # Stem\n",
    "        x = relu_fn(self._bn0(self._conv_stem(inputs)))\n",
    "\n",
    "        # Blocks\n",
    "        for idx, block in enumerate(self._blocks):          \n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Calls extract_features to extract features, applies final linear layer, and returns logits. \"\"\"\n",
    "        \n",
    "        # Convolution layers\n",
    "        x = self.extract_features(inputs).to(device)\n",
    "\n",
    "        # Head\n",
    "        x = relu_fn(self._bn1(self._conv_head(x)))\n",
    "        x = F.adaptive_avg_pool2d(x, 1).squeeze(-1).squeeze(-1)\n",
    "        if self._dropout:\n",
    "            x = F.dropout(x, p=self._dropout, training=self.training)\n",
    "        x = self._fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f480064-6f9e-4464-918f-bbf9200b7dbd",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db7d08ed-e592-4eb7-83fb-f60bec907785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchsummary\n",
    "from torchsummary import summary as summary_\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a5854ed-b093-4022-84c0-d003961d0515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "937e0392-92fa-4fb8-a069-4417aa6409d6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         ZeroPad2d-1           [2, 3, 225, 225]               0\n",
      "Conv2dStaticSamePadding-2          [2, 32, 112, 112]             864\n",
      "       BatchNorm2d-3          [2, 32, 112, 112]              64\n",
      "MemoryEfficientSwish-4          [2, 32, 112, 112]               0\n",
      "         ZeroPad2d-5          [2, 32, 114, 114]               0\n",
      "Conv2dStaticSamePadding-6          [2, 32, 112, 112]             288\n",
      "       BatchNorm2d-7          [2, 32, 112, 112]              64\n",
      "MemoryEfficientSwish-8          [2, 32, 112, 112]               0\n",
      "          Identity-9              [2, 32, 1, 1]               0\n",
      "Conv2dStaticSamePadding-10               [2, 8, 1, 1]             264\n",
      "MemoryEfficientSwish-11               [2, 8, 1, 1]               0\n",
      "         Identity-12               [2, 8, 1, 1]               0\n",
      "Conv2dStaticSamePadding-13              [2, 32, 1, 1]             288\n",
      "         Identity-14          [2, 32, 112, 112]               0\n",
      "Conv2dStaticSamePadding-15          [2, 16, 112, 112]             512\n",
      "      BatchNorm2d-16          [2, 16, 112, 112]              32\n",
      "      MBConvBlock-17          [2, 16, 112, 112]               0\n",
      "         Identity-18          [2, 16, 112, 112]               0\n",
      "Conv2dStaticSamePadding-19          [2, 96, 112, 112]           1,536\n",
      "      BatchNorm2d-20          [2, 96, 112, 112]             192\n",
      "MemoryEfficientSwish-21          [2, 96, 112, 112]               0\n",
      "        ZeroPad2d-22          [2, 96, 113, 113]               0\n",
      "Conv2dStaticSamePadding-23            [2, 96, 56, 56]             864\n",
      "      BatchNorm2d-24            [2, 96, 56, 56]             192\n",
      "MemoryEfficientSwish-25            [2, 96, 56, 56]               0\n",
      "         Identity-26              [2, 96, 1, 1]               0\n",
      "Conv2dStaticSamePadding-27               [2, 4, 1, 1]             388\n",
      "MemoryEfficientSwish-28               [2, 4, 1, 1]               0\n",
      "         Identity-29               [2, 4, 1, 1]               0\n",
      "Conv2dStaticSamePadding-30              [2, 96, 1, 1]             480\n",
      "         Identity-31            [2, 96, 56, 56]               0\n",
      "Conv2dStaticSamePadding-32            [2, 24, 56, 56]           2,304\n",
      "      BatchNorm2d-33            [2, 24, 56, 56]              48\n",
      "      MBConvBlock-34            [2, 24, 56, 56]               0\n",
      "         Identity-35            [2, 24, 56, 56]               0\n",
      "Conv2dStaticSamePadding-36           [2, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-37           [2, 144, 56, 56]             288\n",
      "MemoryEfficientSwish-38           [2, 144, 56, 56]               0\n",
      "        ZeroPad2d-39           [2, 144, 58, 58]               0\n",
      "Conv2dStaticSamePadding-40           [2, 144, 56, 56]           1,296\n",
      "      BatchNorm2d-41           [2, 144, 56, 56]             288\n",
      "MemoryEfficientSwish-42           [2, 144, 56, 56]               0\n",
      "         Identity-43             [2, 144, 1, 1]               0\n",
      "Conv2dStaticSamePadding-44               [2, 6, 1, 1]             870\n",
      "MemoryEfficientSwish-45               [2, 6, 1, 1]               0\n",
      "         Identity-46               [2, 6, 1, 1]               0\n",
      "Conv2dStaticSamePadding-47             [2, 144, 1, 1]           1,008\n",
      "         Identity-48           [2, 144, 56, 56]               0\n",
      "Conv2dStaticSamePadding-49            [2, 24, 56, 56]           3,456\n",
      "      BatchNorm2d-50            [2, 24, 56, 56]              48\n",
      "      MBConvBlock-51            [2, 24, 56, 56]               0\n",
      "         Identity-52            [2, 24, 56, 56]               0\n",
      "Conv2dStaticSamePadding-53           [2, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-54           [2, 144, 56, 56]             288\n",
      "MemoryEfficientSwish-55           [2, 144, 56, 56]               0\n",
      "        ZeroPad2d-56           [2, 144, 59, 59]               0\n",
      "Conv2dStaticSamePadding-57           [2, 144, 28, 28]           3,600\n",
      "      BatchNorm2d-58           [2, 144, 28, 28]             288\n",
      "MemoryEfficientSwish-59           [2, 144, 28, 28]               0\n",
      "         Identity-60             [2, 144, 1, 1]               0\n",
      "Conv2dStaticSamePadding-61               [2, 6, 1, 1]             870\n",
      "MemoryEfficientSwish-62               [2, 6, 1, 1]               0\n",
      "         Identity-63               [2, 6, 1, 1]               0\n",
      "Conv2dStaticSamePadding-64             [2, 144, 1, 1]           1,008\n",
      "         Identity-65           [2, 144, 28, 28]               0\n",
      "Conv2dStaticSamePadding-66            [2, 40, 28, 28]           5,760\n",
      "      BatchNorm2d-67            [2, 40, 28, 28]              80\n",
      "      MBConvBlock-68            [2, 40, 28, 28]               0\n",
      "         Identity-69            [2, 40, 28, 28]               0\n",
      "Conv2dStaticSamePadding-70           [2, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-71           [2, 240, 28, 28]             480\n",
      "MemoryEfficientSwish-72           [2, 240, 28, 28]               0\n",
      "        ZeroPad2d-73           [2, 240, 32, 32]               0\n",
      "Conv2dStaticSamePadding-74           [2, 240, 28, 28]           6,000\n",
      "      BatchNorm2d-75           [2, 240, 28, 28]             480\n",
      "MemoryEfficientSwish-76           [2, 240, 28, 28]               0\n",
      "         Identity-77             [2, 240, 1, 1]               0\n",
      "Conv2dStaticSamePadding-78              [2, 10, 1, 1]           2,410\n",
      "MemoryEfficientSwish-79              [2, 10, 1, 1]               0\n",
      "         Identity-80              [2, 10, 1, 1]               0\n",
      "Conv2dStaticSamePadding-81             [2, 240, 1, 1]           2,640\n",
      "         Identity-82           [2, 240, 28, 28]               0\n",
      "Conv2dStaticSamePadding-83            [2, 40, 28, 28]           9,600\n",
      "      BatchNorm2d-84            [2, 40, 28, 28]              80\n",
      "      MBConvBlock-85            [2, 40, 28, 28]               0\n",
      "         Identity-86            [2, 40, 28, 28]               0\n",
      "Conv2dStaticSamePadding-87           [2, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-88           [2, 240, 28, 28]             480\n",
      "MemoryEfficientSwish-89           [2, 240, 28, 28]               0\n",
      "        ZeroPad2d-90           [2, 240, 29, 29]               0\n",
      "Conv2dStaticSamePadding-91           [2, 240, 14, 14]           2,160\n",
      "      BatchNorm2d-92           [2, 240, 14, 14]             480\n",
      "MemoryEfficientSwish-93           [2, 240, 14, 14]               0\n",
      "         Identity-94             [2, 240, 1, 1]               0\n",
      "Conv2dStaticSamePadding-95              [2, 10, 1, 1]           2,410\n",
      "MemoryEfficientSwish-96              [2, 10, 1, 1]               0\n",
      "         Identity-97              [2, 10, 1, 1]               0\n",
      "Conv2dStaticSamePadding-98             [2, 240, 1, 1]           2,640\n",
      "         Identity-99           [2, 240, 14, 14]               0\n",
      "Conv2dStaticSamePadding-100            [2, 80, 14, 14]          19,200\n",
      "     BatchNorm2d-101            [2, 80, 14, 14]             160\n",
      "     MBConvBlock-102            [2, 80, 14, 14]               0\n",
      "        Identity-103            [2, 80, 14, 14]               0\n",
      "Conv2dStaticSamePadding-104           [2, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-105           [2, 480, 14, 14]             960\n",
      "MemoryEfficientSwish-106           [2, 480, 14, 14]               0\n",
      "       ZeroPad2d-107           [2, 480, 16, 16]               0\n",
      "Conv2dStaticSamePadding-108           [2, 480, 14, 14]           4,320\n",
      "     BatchNorm2d-109           [2, 480, 14, 14]             960\n",
      "MemoryEfficientSwish-110           [2, 480, 14, 14]               0\n",
      "        Identity-111             [2, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-112              [2, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-113              [2, 20, 1, 1]               0\n",
      "        Identity-114              [2, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-115             [2, 480, 1, 1]          10,080\n",
      "        Identity-116           [2, 480, 14, 14]               0\n",
      "Conv2dStaticSamePadding-117            [2, 80, 14, 14]          38,400\n",
      "     BatchNorm2d-118            [2, 80, 14, 14]             160\n",
      "     MBConvBlock-119            [2, 80, 14, 14]               0\n",
      "        Identity-120            [2, 80, 14, 14]               0\n",
      "Conv2dStaticSamePadding-121           [2, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-122           [2, 480, 14, 14]             960\n",
      "MemoryEfficientSwish-123           [2, 480, 14, 14]               0\n",
      "       ZeroPad2d-124           [2, 480, 16, 16]               0\n",
      "Conv2dStaticSamePadding-125           [2, 480, 14, 14]           4,320\n",
      "     BatchNorm2d-126           [2, 480, 14, 14]             960\n",
      "MemoryEfficientSwish-127           [2, 480, 14, 14]               0\n",
      "        Identity-128             [2, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-129              [2, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-130              [2, 20, 1, 1]               0\n",
      "        Identity-131              [2, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-132             [2, 480, 1, 1]          10,080\n",
      "        Identity-133           [2, 480, 14, 14]               0\n",
      "Conv2dStaticSamePadding-134            [2, 80, 14, 14]          38,400\n",
      "     BatchNorm2d-135            [2, 80, 14, 14]             160\n",
      "     MBConvBlock-136            [2, 80, 14, 14]               0\n",
      "        Identity-137            [2, 80, 14, 14]               0\n",
      "Conv2dStaticSamePadding-138           [2, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-139           [2, 480, 14, 14]             960\n",
      "MemoryEfficientSwish-140           [2, 480, 14, 14]               0\n",
      "       ZeroPad2d-141           [2, 480, 18, 18]               0\n",
      "Conv2dStaticSamePadding-142           [2, 480, 14, 14]          12,000\n",
      "     BatchNorm2d-143           [2, 480, 14, 14]             960\n",
      "MemoryEfficientSwish-144           [2, 480, 14, 14]               0\n",
      "        Identity-145             [2, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-146              [2, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-147              [2, 20, 1, 1]               0\n",
      "        Identity-148              [2, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-149             [2, 480, 1, 1]          10,080\n",
      "        Identity-150           [2, 480, 14, 14]               0\n",
      "Conv2dStaticSamePadding-151           [2, 112, 14, 14]          53,760\n",
      "     BatchNorm2d-152           [2, 112, 14, 14]             224\n",
      "     MBConvBlock-153           [2, 112, 14, 14]               0\n",
      "        Identity-154           [2, 112, 14, 14]               0\n",
      "Conv2dStaticSamePadding-155           [2, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-156           [2, 672, 14, 14]           1,344\n",
      "MemoryEfficientSwish-157           [2, 672, 14, 14]               0\n",
      "       ZeroPad2d-158           [2, 672, 18, 18]               0\n",
      "Conv2dStaticSamePadding-159           [2, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-160           [2, 672, 14, 14]           1,344\n",
      "MemoryEfficientSwish-161           [2, 672, 14, 14]               0\n",
      "        Identity-162             [2, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-163              [2, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-164              [2, 28, 1, 1]               0\n",
      "        Identity-165              [2, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-166             [2, 672, 1, 1]          19,488\n",
      "        Identity-167           [2, 672, 14, 14]               0\n",
      "Conv2dStaticSamePadding-168           [2, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-169           [2, 112, 14, 14]             224\n",
      "     MBConvBlock-170           [2, 112, 14, 14]               0\n",
      "        Identity-171           [2, 112, 14, 14]               0\n",
      "Conv2dStaticSamePadding-172           [2, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-173           [2, 672, 14, 14]           1,344\n",
      "MemoryEfficientSwish-174           [2, 672, 14, 14]               0\n",
      "       ZeroPad2d-175           [2, 672, 18, 18]               0\n",
      "Conv2dStaticSamePadding-176           [2, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-177           [2, 672, 14, 14]           1,344\n",
      "MemoryEfficientSwish-178           [2, 672, 14, 14]               0\n",
      "        Identity-179             [2, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-180              [2, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-181              [2, 28, 1, 1]               0\n",
      "        Identity-182              [2, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-183             [2, 672, 1, 1]          19,488\n",
      "        Identity-184           [2, 672, 14, 14]               0\n",
      "Conv2dStaticSamePadding-185           [2, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-186           [2, 112, 14, 14]             224\n",
      "     MBConvBlock-187           [2, 112, 14, 14]               0\n",
      "        Identity-188           [2, 112, 14, 14]               0\n",
      "Conv2dStaticSamePadding-189           [2, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-190           [2, 672, 14, 14]           1,344\n",
      "MemoryEfficientSwish-191           [2, 672, 14, 14]               0\n",
      "       ZeroPad2d-192           [2, 672, 17, 17]               0\n",
      "Conv2dStaticSamePadding-193             [2, 672, 7, 7]          16,800\n",
      "     BatchNorm2d-194             [2, 672, 7, 7]           1,344\n",
      "MemoryEfficientSwish-195             [2, 672, 7, 7]               0\n",
      "        Identity-196             [2, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-197              [2, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-198              [2, 28, 1, 1]               0\n",
      "        Identity-199              [2, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-200             [2, 672, 1, 1]          19,488\n",
      "        Identity-201             [2, 672, 7, 7]               0\n",
      "Conv2dStaticSamePadding-202             [2, 192, 7, 7]         129,024\n",
      "     BatchNorm2d-203             [2, 192, 7, 7]             384\n",
      "     MBConvBlock-204             [2, 192, 7, 7]               0\n",
      "        Identity-205             [2, 192, 7, 7]               0\n",
      "Conv2dStaticSamePadding-206            [2, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-207            [2, 1152, 7, 7]           2,304\n",
      "MemoryEfficientSwish-208            [2, 1152, 7, 7]               0\n",
      "       ZeroPad2d-209          [2, 1152, 11, 11]               0\n",
      "Conv2dStaticSamePadding-210            [2, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-211            [2, 1152, 7, 7]           2,304\n",
      "MemoryEfficientSwish-212            [2, 1152, 7, 7]               0\n",
      "        Identity-213            [2, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-214              [2, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-215              [2, 48, 1, 1]               0\n",
      "        Identity-216              [2, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-217            [2, 1152, 1, 1]          56,448\n",
      "        Identity-218            [2, 1152, 7, 7]               0\n",
      "Conv2dStaticSamePadding-219             [2, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-220             [2, 192, 7, 7]             384\n",
      "     MBConvBlock-221             [2, 192, 7, 7]               0\n",
      "        Identity-222             [2, 192, 7, 7]               0\n",
      "Conv2dStaticSamePadding-223            [2, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-224            [2, 1152, 7, 7]           2,304\n",
      "MemoryEfficientSwish-225            [2, 1152, 7, 7]               0\n",
      "       ZeroPad2d-226          [2, 1152, 11, 11]               0\n",
      "Conv2dStaticSamePadding-227            [2, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-228            [2, 1152, 7, 7]           2,304\n",
      "MemoryEfficientSwish-229            [2, 1152, 7, 7]               0\n",
      "        Identity-230            [2, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-231              [2, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-232              [2, 48, 1, 1]               0\n",
      "        Identity-233              [2, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-234            [2, 1152, 1, 1]          56,448\n",
      "        Identity-235            [2, 1152, 7, 7]               0\n",
      "Conv2dStaticSamePadding-236             [2, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-237             [2, 192, 7, 7]             384\n",
      "     MBConvBlock-238             [2, 192, 7, 7]               0\n",
      "        Identity-239             [2, 192, 7, 7]               0\n",
      "Conv2dStaticSamePadding-240            [2, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-241            [2, 1152, 7, 7]           2,304\n",
      "MemoryEfficientSwish-242            [2, 1152, 7, 7]               0\n",
      "       ZeroPad2d-243          [2, 1152, 11, 11]               0\n",
      "Conv2dStaticSamePadding-244            [2, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-245            [2, 1152, 7, 7]           2,304\n",
      "MemoryEfficientSwish-246            [2, 1152, 7, 7]               0\n",
      "        Identity-247            [2, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-248              [2, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-249              [2, 48, 1, 1]               0\n",
      "        Identity-250              [2, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-251            [2, 1152, 1, 1]          56,448\n",
      "        Identity-252            [2, 1152, 7, 7]               0\n",
      "Conv2dStaticSamePadding-253             [2, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-254             [2, 192, 7, 7]             384\n",
      "     MBConvBlock-255             [2, 192, 7, 7]               0\n",
      "        Identity-256             [2, 192, 7, 7]               0\n",
      "Conv2dStaticSamePadding-257            [2, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-258            [2, 1152, 7, 7]           2,304\n",
      "MemoryEfficientSwish-259            [2, 1152, 7, 7]               0\n",
      "       ZeroPad2d-260            [2, 1152, 9, 9]               0\n",
      "Conv2dStaticSamePadding-261            [2, 1152, 7, 7]          10,368\n",
      "     BatchNorm2d-262            [2, 1152, 7, 7]           2,304\n",
      "MemoryEfficientSwish-263            [2, 1152, 7, 7]               0\n",
      "        Identity-264            [2, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-265              [2, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-266              [2, 48, 1, 1]               0\n",
      "        Identity-267              [2, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-268            [2, 1152, 1, 1]          56,448\n",
      "        Identity-269            [2, 1152, 7, 7]               0\n",
      "Conv2dStaticSamePadding-270             [2, 320, 7, 7]         368,640\n",
      "     BatchNorm2d-271             [2, 320, 7, 7]             640\n",
      "     MBConvBlock-272             [2, 320, 7, 7]               0\n",
      "        Identity-273             [2, 320, 7, 7]               0\n",
      "Conv2dStaticSamePadding-274            [2, 1280, 7, 7]         409,600\n",
      "     BatchNorm2d-275            [2, 1280, 7, 7]           2,560\n",
      "MemoryEfficientSwish-276            [2, 1280, 7, 7]               0\n",
      "AdaptiveAvgPool2d-277            [2, 1280, 1, 1]               0\n",
      "         Dropout-278                  [2, 1280]               0\n",
      "          Linear-279                     [2, 4]           5,124\n",
      "================================================================\n",
      "Total params: 4,012,672\n",
      "Trainable params: 4,012,672\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.15\n",
      "Forward/backward pass size (MB): 425.58\n",
      "Params size (MB): 15.31\n",
      "Estimated Total Size (MB): 442.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet.from_name('efficientnet-b0', \n",
    "                               num_classes=4,\n",
    "                               dropout_rate=0.5).to(device)\n",
    "summary_(model, (3, 224, 224), batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f556bb4-c2c8-4daf-aaea-4ebae2f2f3ca",
   "metadata": {},
   "source": [
    "### Neptune AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6f918a4-4376-48ce-a074-6fdca51b3553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06521e48-9c2d-4b8c-b88b-ba7bec2926e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2018668/590607257.py:1: NeptuneWarning: To avoid unintended consumption of logging hours during interactive sessions, the following monitoring options are disabled unless set to 'True' when initializing the run: 'capture_stdout', 'capture_stderr', and 'capture_hardware_metrics'.\n",
      "  run = neptune.init_run(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/leehe228/plant-pathology/e/P001-41\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init_run(\n",
    "    project=\"leehe228/plant-pathology\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI5MTRmYjRlNC0zODFlLTQ0ODItODY1MC1hZGQ0YTRhNDNlZjIifQ==\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eae25d-b0f7-411b-96d5-9cd4b7d2d4c0",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae81c6eb-0dd0-4221-8eb8-080d0c32dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # lr 1e-3 ~ 1e-6\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"input_size\": 3 * 244 * 244,\n",
    "    \"num_epoch\": 25,\n",
    "    \"n_classes\": 4,\n",
    "    \"optimizer\": 'Adam',\n",
    "    \"criterion\": 'CrossEntropyLoss',\n",
    "    \"preproc_type\": \"transform\",\n",
    "    \"model\":\"EfficientNetB0\",\n",
    "    \"library\":\"PyTorch\",\n",
    "    \"dropout\":0.5,\n",
    "    \"device\": str(device)\n",
    "}\n",
    "# add parameters\n",
    "run[\"parameters\"] = params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071267ce-8839-4fe0-a488-49d80c8ecc5f",
   "metadata": {},
   "source": [
    "### Optimizer and Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f55a9338-e73a-484e-9459-a4b1413e0c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1289103e-4fea-462f-a447-6e129fb1465c",
   "metadata": {},
   "source": [
    "### Metric: ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c225bd0-aff1-4811-85b3-905203006dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics.aggregation.auc import AUC\n",
    "from torchmetrics import AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "516a1301-82ef-4d10-9c9c-3ae4fd85a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric_auc = AUC()\n",
    "metric_auc = AUROC(task=\"multiclass\", num_classes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125054e5-e8a8-4fe0-b7b4-0c2c8bd1807c",
   "metadata": {},
   "source": [
    "### Learning Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bbaf0b37-38b4-4e71-8689-3f8b6ba3171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T_max : number of iter\n",
    "# eta_min : min value of learning rate\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, \n",
    "#                                                        T_max=5, \n",
    "#                                                        eta_min=1e-6,\n",
    "#                                                        last_epoch=-1,\n",
    "#                                                        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3023b84e-dc70-4b9d-b88b-065ad746d3c9",
   "metadata": {},
   "source": [
    "### Training Funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e0c049c-ab50-4c48-8eee-5af428cfc2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8aa6652-7add-4773-952d-96bf3876579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, params, criterion, optimizer):\n",
    "    \n",
    "    optim_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    run[\"train/epoch/lr\"].append(optim_lr)\n",
    "        \n",
    "    for epoch in range(0, params['num_epoch']):\n",
    "        print(f\"epoch {epoch + 1}.\")\n",
    "        \n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            # data, label 분리\n",
    "            x_train, y_train = data\n",
    "            \n",
    "            # y_train to one_hot_encoding\n",
    "            labels = F.one_hot(y_train, num_classes=4).double()\n",
    "        \n",
    "            inputs = x_train.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # 이전 batch에서 계산된 가중치 초기화\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + back propagation\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            # calculate loss, acc, roc auc\n",
    "            train_loss = criterion(outputs, labels)\n",
    "            train_acc = (torch.sum(preds == torch.argmax(labels, dim=1))).sum().item() / len(inputs) \n",
    "            train_auc = metric_auc(labels, preds).item()\n",
    "            \n",
    "            print(f\"batch {i}/{n_train_data//BATCH_SIZE} acc:{train_acc}, auc:{train_auc}, loss:{train_loss}\")\n",
    "            \n",
    "            # training batch loss and accuracy, auc\n",
    "            run[\"train/batch/loss\"].append(train_loss)\n",
    "            run[\"train/batch/acc\"].append(train_acc)\n",
    "            run['train/batch/auc'].append(train_auc)\n",
    "            \n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            metric_auc.reset()\n",
    "        \n",
    "        # empty GPU RAM\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # test accuracy\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        accuracy = []\n",
    "        auc = []\n",
    "        losses = []\n",
    "        \n",
    "        for i, data in enumerate(test_dataloader, 0):\n",
    "            x_train, y_train = data\n",
    "            \n",
    "            # y_train to one_hot_encoding\n",
    "            labels = F.one_hot(y_train, num_classes=4).double()\n",
    "        \n",
    "            inputs = x_train.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (torch.sum(preds == torch.argmax(labels, dim=1))).sum().item()\n",
    "            test_loss = criterion(outputs, labels).item()\n",
    "            test_auc = (metric_auc(labels, preds)).item()\n",
    "            test_accuracy = correct / total\n",
    "        \n",
    "            run[\"test/batch/loss\"].append(test_loss)\n",
    "            run[\"test/batch/acc\"].append(test_accuracy)\n",
    "            run['test/batch/auc'].append(test_auc)\n",
    "            \n",
    "            accuracy.append(test_accuracy)\n",
    "            auc.append(test_auc)\n",
    "            losses.append(test_loss)\n",
    "            metric_auc.reset()\n",
    "        \n",
    "        # empty GPU RAM\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        num_epochs = params[\"num_epoch\"]\n",
    "        print(f\"epoch: {epoch+1}/{num_epochs}, Test Loss: {np.mean(losses)}, Test Acc: {np.mean(accuracy)}, Test AUC: {np.mean(auc)}\")\n",
    "        print(\"\\n\")\n",
    "        print(\"=\"*30)\n",
    "        \n",
    "        run[\"test/epoch/loss\"].append(np.mean(losses))\n",
    "        run[\"test/epoch/acc\"].append(np.mean(accuracy))\n",
    "        run['test/epoch/auc'].append(np.mean(auc))\n",
    "        \n",
    "        # scheduler\n",
    "        # scheduler.step(np.mean(auc))\n",
    "        \n",
    "        optim_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print(f\"learning rate epoch {epoch} : {optim_lr}\")\n",
    "        run[\"train/epoch/lr\"].append(optim_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "303c0e71-c384-40f5-b2ed-f516c1d8bfce",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/hackthon2/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0/28 acc:0.21875, auc:0.4857335090637207, loss:1.4361334573477507\n",
      "batch 1/28 acc:0.375, auc:0.5527283549308777, loss:1.405039700679481\n",
      "batch 2/28 acc:0.265625, auc:0.4738917946815491, loss:1.435982882976532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/hackthon2/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3/28 acc:0.4375, auc:0.4408894032239914, loss:1.2980095997918397\n",
      "batch 4/28 acc:0.328125, auc:0.49870800971984863, loss:1.343501664698124\n",
      "batch 5/28 acc:0.4375, auc:0.4614075720310211, loss:1.3263648296706378\n",
      "batch 6/28 acc:0.40625, auc:0.41284826397895813, loss:1.3611409915611148\n",
      "batch 7/28 acc:0.421875, auc:0.4185192584991455, loss:1.3090643286705017\n",
      "batch 8/28 acc:0.34375, auc:0.4055669903755188, loss:1.3995352378115058\n",
      "batch 9/28 acc:0.3125, auc:0.36064836382865906, loss:1.3924778664950281\n",
      "batch 10/28 acc:0.40625, auc:0.39348389208316803, loss:1.3506021574139595\n",
      "batch 11/28 acc:0.40625, auc:0.49998974800109863, loss:1.2725182776339352\n",
      "batch 12/28 acc:0.5, auc:0.6238123178482056, loss:1.263876212760806\n",
      "batch 13/28 acc:0.4375, auc:0.5898457765579224, loss:1.2849973663687706\n",
      "batch 14/28 acc:0.390625, auc:0.4030410349369049, loss:1.3066226574592292\n",
      "batch 15/28 acc:0.546875, auc:0.606717050075531, loss:1.2313218768686056\n",
      "batch 16/28 acc:0.390625, auc:0.41529643535614014, loss:1.2900232586544007\n",
      "batch 17/28 acc:0.390625, auc:0.5373309850692749, loss:1.348984755575657\n",
      "batch 18/28 acc:0.421875, auc:0.459307000041008, loss:1.2660798206925392\n",
      "batch 19/28 acc:0.328125, auc:0.39236578345298767, loss:1.308732493314892\n",
      "batch 20/28 acc:0.421875, auc:0.4512593299150467, loss:1.213215502910316\n",
      "batch 21/28 acc:0.5, auc:0.4879167228937149, loss:1.2262762088794261\n",
      "epoch: 1/25, Test Loss: 1.2778027780692687, Test Acc: 0.41138999127494763, Test AUC: 0.4480503126978874\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 0 : 0.0001\n",
      "epoch 2.\n",
      "batch 0/28 acc:0.390625, auc:0.4252292066812515, loss:1.3106148792430758\n",
      "batch 1/28 acc:0.40625, auc:0.41157692670822144, loss:1.241899921093136\n",
      "batch 2/28 acc:0.453125, auc:0.5755801200866699, loss:1.216986330691725\n",
      "batch 3/28 acc:0.46875, auc:0.63374924659729, loss:1.1983374077826738\n",
      "batch 4/28 acc:0.328125, auc:0.48719385266304016, loss:1.3887071376666427\n",
      "batch 5/28 acc:0.390625, auc:0.42653021216392517, loss:1.2303885342553258\n",
      "batch 6/28 acc:0.375, auc:0.572938084602356, loss:1.2972759287804365\n",
      "batch 7/28 acc:0.515625, auc:0.583027184009552, loss:1.2445461116731167\n",
      "batch 8/28 acc:0.515625, auc:0.5053423047065735, loss:1.230624832212925\n",
      "batch 9/28 acc:0.40625, auc:0.5299941301345825, loss:1.2649561646394432\n",
      "batch 10/28 acc:0.453125, auc:0.47925831377506256, loss:1.2727902918122709\n",
      "batch 11/28 acc:0.578125, auc:0.5345363467931747, loss:1.1364448019303381\n",
      "batch 12/28 acc:0.390625, auc:0.40559637546539307, loss:1.2206219034269452\n",
      "batch 13/28 acc:0.4375, auc:0.4464876502752304, loss:1.2028245329856873\n",
      "batch 14/28 acc:0.421875, auc:0.426311731338501, loss:1.2543525760993361\n",
      "batch 15/28 acc:0.3125, auc:0.38049399852752686, loss:1.3545959275215864\n",
      "batch 16/28 acc:0.546875, auc:0.525833785533905, loss:1.1441511744633317\n",
      "batch 17/28 acc:0.390625, auc:0.44838325679302216, loss:1.2445280901156366\n",
      "batch 18/28 acc:0.359375, auc:0.44402770698070526, loss:1.3707820414565504\n",
      "batch 19/28 acc:0.515625, auc:0.48704321682453156, loss:1.1553963529877365\n",
      "batch 20/28 acc:0.34375, auc:0.3866725116968155, loss:1.272032167762518\n",
      "batch 21/28 acc:0.4375, auc:0.4418904036283493, loss:1.2080008080229163\n",
      "epoch: 2/25, Test Loss: 1.2960800317073748, Test Acc: 0.37678697814014794, Test AUC: 0.4129305414178155\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 1 : 0.0001\n",
      "epoch 3.\n",
      "batch 0/28 acc:0.421875, auc:0.4231986105442047, loss:1.1300859744660556\n",
      "batch 1/28 acc:0.40625, auc:0.41801618039608, loss:1.3161713490262628\n",
      "batch 2/28 acc:0.5, auc:0.477510467171669, loss:1.1308440627763048\n",
      "batch 3/28 acc:0.453125, auc:0.44190239161252975, loss:1.230962136760354\n",
      "batch 4/28 acc:0.5, auc:0.4656006395816803, loss:1.0887349167605862\n",
      "batch 5/28 acc:0.40625, auc:0.444857656955719, loss:1.188271677470766\n",
      "batch 6/28 acc:0.46875, auc:0.4587477594614029, loss:1.2024409065925283\n",
      "batch 7/28 acc:0.53125, auc:0.5288860648870468, loss:1.1485363319516182\n",
      "batch 8/28 acc:0.5, auc:0.49265851080417633, loss:1.1135092750191689\n",
      "batch 9/28 acc:0.421875, auc:0.43274030089378357, loss:1.2316228626295924\n",
      "batch 10/28 acc:0.46875, auc:0.4882926791906357, loss:1.1291494409088045\n",
      "batch 11/28 acc:0.40625, auc:0.259424589574337, loss:1.1739928408060223\n",
      "batch 12/28 acc:0.375, auc:0.41947316378355026, loss:1.1933933105319738\n",
      "batch 13/28 acc:0.453125, auc:0.4730582535266876, loss:1.22535859933123\n",
      "batch 14/28 acc:0.3125, auc:0.41555600613355637, loss:1.2865297643002123\n",
      "batch 15/28 acc:0.46875, auc:0.517018660902977, loss:1.1367054618895054\n",
      "batch 16/28 acc:0.40625, auc:0.49797503650188446, loss:1.21474109205883\n",
      "batch 17/28 acc:0.375, auc:0.4935515224933624, loss:1.2112037502229214\n",
      "batch 18/28 acc:0.4375, auc:0.4623999744653702, loss:1.14151013456285\n",
      "batch 19/28 acc:0.421875, auc:0.4175928235054016, loss:1.253883940167725\n",
      "batch 20/28 acc:0.515625, auc:0.5098313391208649, loss:1.1256917000282556\n",
      "batch 21/28 acc:0.578125, auc:0.5332310199737549, loss:1.2244852669537067\n",
      "epoch: 3/25, Test Loss: 1.2230900811158458, Test Acc: 0.4273691378730255, Test AUC: 0.4574145112525333\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 2 : 0.0001\n",
      "epoch 4.\n",
      "batch 0/28 acc:0.578125, auc:0.5305908918380737, loss:1.0773167260922492\n",
      "batch 1/28 acc:0.515625, auc:0.6274247169494629, loss:1.2065575495362282\n",
      "batch 2/28 acc:0.5, auc:0.48617784678936005, loss:1.1074384753592312\n",
      "batch 3/28 acc:0.46875, auc:0.4696701318025589, loss:1.1619575200602412\n",
      "batch 4/28 acc:0.609375, auc:0.5495352894067764, loss:1.0350421667681076\n",
      "batch 5/28 acc:0.59375, auc:0.5375045090913773, loss:1.0612988907378167\n",
      "batch 6/28 acc:0.53125, auc:0.49532826244831085, loss:1.0563574133557267\n",
      "batch 7/28 acc:0.484375, auc:0.46879731118679047, loss:1.1244436409324408\n",
      "batch 8/28 acc:0.5, auc:0.4708208441734314, loss:1.274145139468601\n",
      "batch 9/28 acc:0.484375, auc:0.47807927429676056, loss:1.1121983631746843\n",
      "batch 10/28 acc:0.484375, auc:0.4690970629453659, loss:1.1205573521438055\n",
      "batch 11/28 acc:0.46875, auc:0.4556698054075241, loss:1.2030229514930397\n",
      "batch 12/28 acc:0.5625, auc:0.5309474915266037, loss:1.0002942375140265\n",
      "batch 13/28 acc:0.453125, auc:0.4746920168399811, loss:1.1277430813934188\n",
      "batch 14/28 acc:0.515625, auc:0.4934990555047989, loss:1.0423253024928272\n",
      "batch 15/28 acc:0.5, auc:0.4774511456489563, loss:1.1242207437753677\n",
      "batch 16/28 acc:0.421875, auc:0.43159301578998566, loss:1.1545140274101868\n",
      "batch 17/28 acc:0.5625, auc:0.5186483263969421, loss:1.115229760762304\n",
      "batch 18/28 acc:0.4375, auc:0.4457918405532837, loss:1.1567707942740526\n",
      "batch 19/28 acc:0.484375, auc:0.46096478402614594, loss:1.044148805201985\n",
      "batch 20/28 acc:0.5, auc:0.47533829510211945, loss:1.0808035818627104\n",
      "batch 21/28 acc:0.5625, auc:0.5035297125577927, loss:1.0807569526950829\n",
      "epoch: 4/25, Test Loss: 1.2130605448672378, Test Acc: 0.43907446400212197, Test AUC: 0.45251721956513147\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 3 : 0.0001\n",
      "epoch 5.\n",
      "batch 0/28 acc:0.5625, auc:0.49822451174259186, loss:1.0124687729403377\n",
      "batch 1/28 acc:0.5625, auc:0.5164954215288162, loss:1.077684748917818\n",
      "batch 2/28 acc:0.625, auc:0.551534965634346, loss:0.9271812574588694\n",
      "batch 3/28 acc:0.640625, auc:0.5596358329057693, loss:0.9489970473805442\n",
      "batch 4/28 acc:0.6875, auc:0.5829125344753265, loss:0.92710917792283\n",
      "batch 5/28 acc:0.5, auc:0.47668032348155975, loss:1.0566495715174824\n",
      "batch 6/28 acc:0.609375, auc:0.5371910184621811, loss:1.0273370726499707\n",
      "batch 7/28 acc:0.515625, auc:0.49915532767772675, loss:1.1351324139395729\n",
      "batch 8/28 acc:0.546875, auc:0.5077532529830933, loss:1.0379716587194707\n",
      "batch 9/28 acc:0.46875, auc:0.7288434505462646, loss:1.1007811882300302\n",
      "batch 10/28 acc:0.578125, auc:0.5500398278236389, loss:1.0492790740681812\n",
      "batch 11/28 acc:0.609375, auc:0.5756731629371643, loss:0.9819806221639737\n",
      "batch 12/28 acc:0.625, auc:0.5597505867481232, loss:0.9483064879605081\n",
      "batch 13/28 acc:0.5, auc:0.5046836733818054, loss:1.0821082982583903\n",
      "batch 14/28 acc:0.5625, auc:0.5438244193792343, loss:1.003854312824842\n",
      "batch 15/28 acc:0.5625, auc:0.5188225358724594, loss:0.9951380901038647\n",
      "batch 16/28 acc:0.515625, auc:0.4897391349077225, loss:1.0706082207034342\n",
      "batch 17/28 acc:0.59375, auc:0.5330100655555725, loss:0.9982622988463845\n",
      "batch 18/28 acc:0.609375, auc:0.5676358342170715, loss:0.9383759147895034\n",
      "batch 19/28 acc:0.625, auc:0.5537361353635788, loss:0.9826661783445161\n",
      "batch 20/28 acc:0.578125, auc:0.5377670675516129, loss:0.9824411373701878\n",
      "batch 21/28 acc:0.578125, auc:0.5370919108390808, loss:1.1113146818242967\n",
      "epoch: 5/25, Test Loss: 1.223111987727779, Test Acc: 0.4589454271370302, Test AUC: 0.4616088213568384\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 4 : 0.0001\n",
      "epoch 6.\n",
      "batch 0/28 acc:0.625, auc:0.5719758421182632, loss:0.9882950693718158\n",
      "batch 1/28 acc:0.5, auc:0.5078248977661133, loss:1.024872263893485\n",
      "batch 2/28 acc:0.6875, auc:0.588094025850296, loss:0.9373495527725026\n",
      "batch 3/28 acc:0.703125, auc:0.5934722125530243, loss:0.815031841484597\n",
      "batch 4/28 acc:0.625, auc:0.5537102967500687, loss:0.9666363515425473\n",
      "batch 5/28 acc:0.59375, auc:0.5306756049394608, loss:0.9320921972685028\n",
      "batch 6/28 acc:0.625, auc:0.5730600953102112, loss:1.0149354843342735\n",
      "batch 7/28 acc:0.578125, auc:0.5272006839513779, loss:0.9865295201307163\n",
      "batch 8/28 acc:0.6875, auc:0.5940158069133759, loss:0.8819945477735018\n",
      "batch 9/28 acc:0.625, auc:0.5598651021718979, loss:0.9522802247665823\n",
      "batch 10/28 acc:0.625, auc:0.5500984489917755, loss:0.9089645990025019\n",
      "batch 11/28 acc:0.6875, auc:0.5906125605106354, loss:0.7668539909209358\n",
      "batch 12/28 acc:0.546875, auc:0.5149590522050858, loss:1.118372512406495\n",
      "batch 13/28 acc:0.609375, auc:0.5472733974456787, loss:0.8694915104970278\n",
      "batch 14/28 acc:0.65625, auc:0.5653286725282669, loss:0.8776628360515133\n",
      "batch 15/28 acc:0.71875, auc:0.6124362647533417, loss:0.7673423675278173\n",
      "batch 16/28 acc:0.703125, auc:0.595235675573349, loss:0.7554324367501977\n",
      "batch 17/28 acc:0.6875, auc:0.5981650054454803, loss:0.8851869731424813\n",
      "batch 18/28 acc:0.734375, auc:0.5968569815158844, loss:0.72961008672155\n",
      "batch 19/28 acc:0.625, auc:0.546317920088768, loss:0.8743854814529186\n",
      "batch 20/28 acc:0.65625, auc:0.5693631619215012, loss:0.866591187155791\n",
      "batch 21/28 acc:0.59375, auc:0.5307943224906921, loss:0.9399169492535293\n",
      "epoch: 6/25, Test Loss: 1.320675733657977, Test Acc: 0.4870829395204703, Test AUC: 0.46548864753408864\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 5 : 0.0001\n",
      "epoch 7.\n",
      "batch 0/28 acc:0.6875, auc:0.8324460983276367, loss:0.7646832882164745\n",
      "batch 1/28 acc:0.65625, auc:0.5879051983356476, loss:0.9000172501873749\n",
      "batch 2/28 acc:0.6875, auc:0.5817482173442841, loss:0.8243552846597595\n",
      "batch 3/28 acc:0.65625, auc:0.5568164587020874, loss:1.0404209025145974\n",
      "batch 4/28 acc:0.6875, auc:0.5873855501413345, loss:0.8368413945026987\n",
      "batch 5/28 acc:0.765625, auc:0.6300621628761292, loss:0.7909810257115168\n",
      "batch 6/28 acc:0.765625, auc:0.6188625395298004, loss:0.6678963197118719\n",
      "batch 7/28 acc:0.796875, auc:0.6398528963327408, loss:0.6667614764046448\n",
      "batch 8/28 acc:0.765625, auc:0.6350057423114777, loss:0.6431035067580524\n",
      "batch 9/28 acc:0.75, auc:0.6322954148054123, loss:0.7293115602535636\n",
      "batch 10/28 acc:0.65625, auc:0.5704231858253479, loss:0.89400545447279\n",
      "batch 11/28 acc:0.78125, auc:0.6377765536308289, loss:0.6574663095780124\n",
      "batch 12/28 acc:0.6875, auc:0.5914487838745117, loss:0.8589709213028982\n",
      "batch 13/28 acc:0.734375, auc:0.6144553571939468, loss:0.7802258530355175\n",
      "batch 14/28 acc:0.734375, auc:0.599359929561615, loss:0.6540385582802628\n",
      "batch 15/28 acc:0.78125, auc:0.6351765394210815, loss:0.6736116256906826\n",
      "batch 16/28 acc:0.703125, auc:0.5895033180713654, loss:0.739139871648149\n",
      "batch 17/28 acc:0.6875, auc:0.5893479585647583, loss:0.7464066372394882\n",
      "batch 18/28 acc:0.78125, auc:0.6277096420526505, loss:0.756563288842699\n",
      "batch 19/28 acc:0.609375, auc:0.5544198751449585, loss:1.0179780252219643\n",
      "batch 20/28 acc:0.671875, auc:0.5654002130031586, loss:0.6991723874307354\n",
      "batch 21/28 acc:0.71875, auc:0.6158854216337204, loss:0.7853014748889109\n",
      "epoch: 7/25, Test Loss: 1.349057803819464, Test Acc: 0.4611368916962342, Test AUC: 0.4588959687812762\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 6 : 0.0001\n",
      "epoch 8.\n",
      "batch 0/28 acc:0.703125, auc:0.6153484284877777, loss:0.7785303522614413\n",
      "batch 1/28 acc:0.734375, auc:0.6025870144367218, loss:0.6863888903665156\n",
      "batch 2/28 acc:0.78125, auc:0.636853039264679, loss:0.5672208345185936\n",
      "batch 3/28 acc:0.75, auc:0.6154356002807617, loss:0.6103296299888825\n",
      "batch 4/28 acc:0.609375, auc:0.5503552556037903, loss:0.8229809884069255\n",
      "batch 5/28 acc:0.796875, auc:0.6471210569143295, loss:0.6934618439550491\n",
      "batch 6/28 acc:0.796875, auc:0.640566810965538, loss:0.5940531387309136\n",
      "batch 7/28 acc:0.8125, auc:0.6350132673978806, loss:0.5339344251005969\n",
      "batch 8/28 acc:0.78125, auc:0.6368560642004013, loss:0.7246908364941191\n",
      "batch 9/28 acc:0.6875, auc:0.5813415348529816, loss:1.0027877308948518\n",
      "batch 10/28 acc:0.765625, auc:0.8776947259902954, loss:0.774963235229734\n",
      "batch 11/28 acc:0.84375, auc:0.6771557927131653, loss:0.4793597044563285\n",
      "batch 12/28 acc:0.625, auc:0.5471751689910889, loss:0.9543569193483563\n",
      "batch 13/28 acc:0.75, auc:0.611868754029274, loss:0.7307775410827162\n",
      "batch 14/28 acc:0.765625, auc:0.6274208277463913, loss:0.6517916202305969\n",
      "batch 15/28 acc:0.796875, auc:0.6419525295495987, loss:0.6008863251654475\n",
      "batch 16/28 acc:0.75, auc:0.619122177362442, loss:0.6393226990581979\n",
      "batch 17/28 acc:0.734375, auc:0.8510046005249023, loss:0.6435638251914497\n",
      "batch 18/28 acc:0.75, auc:0.623230054974556, loss:0.6195593266945707\n",
      "batch 19/28 acc:0.640625, auc:0.5681023299694061, loss:0.8460289604168878\n",
      "batch 20/28 acc:0.671875, auc:0.5830973982810974, loss:0.7759045520379004\n",
      "batch 21/28 acc:0.78125, auc:0.6536935269832611, loss:0.6261500392733979\n",
      "epoch: 8/25, Test Loss: 1.4995566751551286, Test Acc: 0.4374845587751055, Test AUC: 0.4511384330689907\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 7 : 0.0001\n",
      "epoch 9.\n",
      "batch 0/28 acc:0.859375, auc:0.6896142810583115, loss:0.46113587761328745\n",
      "batch 1/28 acc:0.765625, auc:0.6427489072084427, loss:0.6401594322012585\n",
      "batch 2/28 acc:0.703125, auc:0.615982174873352, loss:0.6945644259976689\n",
      "batch 3/28 acc:0.75, auc:0.6216787695884705, loss:0.6752033898891341\n",
      "batch 4/28 acc:0.703125, auc:0.5858133882284164, loss:0.7674529020864611\n",
      "batch 5/28 acc:0.6875, auc:0.5890619605779648, loss:0.7346149128120487\n",
      "batch 6/28 acc:0.765625, auc:0.6400291323661804, loss:0.5834642423083096\n",
      "batch 7/28 acc:0.703125, auc:0.6009749919176102, loss:0.6632709254847668\n",
      "batch 8/28 acc:0.8125, auc:0.899275541305542, loss:0.5683578424185498\n",
      "batch 9/28 acc:0.828125, auc:0.6578847914934158, loss:0.6289085073440219\n",
      "batch 10/28 acc:0.84375, auc:0.6696081161499023, loss:0.47475820285671944\n",
      "batch 11/28 acc:0.734375, auc:0.8534403443336487, loss:0.6694702237496131\n",
      "batch 12/28 acc:0.71875, auc:0.8445708751678467, loss:0.6579511502885396\n",
      "batch 13/28 acc:0.6875, auc:0.6040048450231552, loss:0.7719713896913163\n",
      "batch 14/28 acc:0.8125, auc:0.65091672539711, loss:0.47790025098288424\n",
      "batch 15/28 acc:0.828125, auc:0.9054465889930725, loss:0.5056462608683887\n",
      "batch 16/28 acc:0.90625, auc:0.6993548274040222, loss:0.30605568747887446\n",
      "batch 17/28 acc:0.8125, auc:0.6567708402872086, loss:0.5064231386013489\n",
      "batch 18/28 acc:0.734375, auc:0.6204430609941483, loss:0.6175908864347548\n",
      "batch 19/28 acc:0.859375, auc:0.6815985888242722, loss:0.4171207651907025\n",
      "batch 20/28 acc:0.8125, auc:0.6354618072509766, loss:0.6148945784063926\n",
      "batch 21/28 acc:0.734375, auc:0.6167364418506622, loss:0.6839534134123824\n",
      "epoch: 9/25, Test Loss: 1.6812934249296219, Test Acc: 0.43519657294108466, Test AUC: 0.43988567184318195\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 8 : 0.0001\n",
      "epoch 10.\n",
      "batch 0/28 acc:0.859375, auc:0.6799606531858444, loss:0.44201150295157277\n",
      "batch 1/28 acc:0.859375, auc:0.9245421886444092, loss:0.38572345004467934\n",
      "batch 2/28 acc:0.875, auc:0.693247601389885, loss:0.44083354312579104\n",
      "batch 3/28 acc:0.75, auc:0.8708878755569458, loss:0.6730121018149475\n",
      "batch 4/28 acc:0.859375, auc:0.6766634583473206, loss:0.44875746999449007\n",
      "batch 5/28 acc:0.875, auc:0.6830801069736481, loss:0.38630861156612184\n",
      "batch 6/28 acc:0.859375, auc:0.9287956953048706, loss:0.40978117402104886\n",
      "batch 7/28 acc:0.828125, auc:0.6634390503168106, loss:0.5120250191703235\n",
      "batch 8/28 acc:0.84375, auc:0.6739571839570999, loss:0.4035830473790156\n",
      "batch 9/28 acc:0.828125, auc:0.6688696593046188, loss:0.6290766384099697\n",
      "batch 10/28 acc:0.75, auc:0.6152383238077164, loss:0.6227497369081902\n",
      "batch 11/28 acc:0.734375, auc:0.8603667616844177, loss:0.6558423268306726\n",
      "batch 12/28 acc:0.890625, auc:0.6964540630578995, loss:0.24424815870588645\n",
      "batch 13/28 acc:0.796875, auc:0.6506468206644058, loss:0.4678680714088159\n",
      "batch 14/28 acc:0.8125, auc:0.6561347395181656, loss:0.6260460468565725\n",
      "batch 15/28 acc:0.78125, auc:0.6383482813835144, loss:0.5503945898196889\n",
      "batch 16/28 acc:0.84375, auc:0.6663402020931244, loss:0.360362742774214\n",
      "batch 17/28 acc:0.78125, auc:0.638213261961937, loss:0.46175380738441163\n",
      "batch 18/28 acc:0.828125, auc:0.6560611724853516, loss:0.5387374330706081\n",
      "batch 19/28 acc:0.78125, auc:0.6330370604991913, loss:0.5524100293396259\n",
      "batch 20/28 acc:0.859375, auc:0.6588647067546844, loss:0.4713682088645328\n",
      "batch 21/28 acc:0.875, auc:0.9293074607849121, loss:0.35769407250523955\n",
      "epoch: 10/25, Test Loss: 1.7497241442602118, Test Acc: 0.4420951000421694, Test AUC: 0.47560911354693497\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 9 : 0.0001\n",
      "epoch 11.\n",
      "batch 0/28 acc:0.765625, auc:0.8722280263900757, loss:0.583645415181195\n",
      "batch 1/28 acc:0.875, auc:0.6877135932445526, loss:0.36741817598021953\n",
      "batch 2/28 acc:0.859375, auc:0.9248304963111877, loss:0.46176743653794716\n",
      "batch 3/28 acc:0.875, auc:0.9327700734138489, loss:0.4135182442097971\n",
      "batch 4/28 acc:0.8125, auc:0.8960258960723877, loss:0.5007765673957039\n",
      "batch 5/28 acc:0.890625, auc:0.6934072375297546, loss:0.4246599569482896\n",
      "batch 6/28 acc:0.875, auc:0.682836502790451, loss:0.39465124814341834\n",
      "batch 7/28 acc:0.84375, auc:0.8399202823638916, loss:0.3630085963648071\n",
      "batch 8/28 acc:0.78125, auc:0.6383313834667206, loss:0.5254292445384863\n",
      "batch 9/28 acc:0.84375, auc:0.9156959056854248, loss:0.45524167182838937\n",
      "batch 10/28 acc:0.859375, auc:0.9228540658950806, loss:0.33323260385532194\n",
      "batch 11/28 acc:0.859375, auc:0.6701131165027618, loss:0.39146256722642647\n",
      "batch 12/28 acc:0.875, auc:0.9277713298797607, loss:0.34269278720330476\n",
      "batch 13/28 acc:0.8125, auc:0.9009177684783936, loss:0.455645561488609\n",
      "batch 14/28 acc:0.8125, auc:0.9009954333305359, loss:0.48951614170672997\n",
      "batch 15/28 acc:0.828125, auc:0.6515676528215408, loss:0.3960044152520368\n",
      "batch 16/28 acc:0.828125, auc:0.9044440984725952, loss:0.4161373461724338\n",
      "batch 17/28 acc:0.859375, auc:0.925688624382019, loss:0.4580525052047051\n",
      "batch 18/28 acc:0.828125, auc:0.8961338996887207, loss:0.4595943526028492\n",
      "batch 19/28 acc:0.875, auc:0.8192628622055054, loss:0.3255279658815198\n",
      "batch 20/28 acc:0.875, auc:0.936153769493103, loss:0.3696860390935228\n",
      "batch 21/28 acc:0.875, auc:0.6917140036821365, loss:0.4797833252107466\n",
      "epoch: 11/25, Test Loss: 1.7608653469517115, Test Acc: 0.451730270269181, Test AUC: 0.4649752080440521\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 10 : 0.0001\n",
      "epoch 12.\n",
      "batch 0/28 acc:0.859375, auc:0.9325096607208252, loss:0.41008034154650375\n",
      "batch 1/28 acc:0.84375, auc:0.9121489524841309, loss:0.3659280984128088\n",
      "batch 2/28 acc:0.875, auc:0.9253672361373901, loss:0.3804138749828905\n",
      "batch 3/28 acc:0.84375, auc:0.6636350154876709, loss:0.4661614269864067\n",
      "batch 4/28 acc:0.859375, auc:0.9210806488990784, loss:0.3579538392124064\n",
      "batch 5/28 acc:0.875, auc:0.9285444021224976, loss:0.3388538324871888\n",
      "batch 6/28 acc:0.84375, auc:0.9193301796913147, loss:0.33948897068870565\n",
      "batch 7/28 acc:0.9375, auc:0.9603866338729858, loss:0.18701720994067728\n",
      "batch 8/28 acc:0.796875, auc:0.8527986407279968, loss:0.5374149512377571\n",
      "batch 9/28 acc:0.84375, auc:0.9033908843994141, loss:0.46032311602215215\n",
      "batch 10/28 acc:0.90625, auc:0.7009901106357574, loss:0.3268315535535571\n",
      "batch 11/28 acc:0.875, auc:0.9324724078178406, loss:0.27823750769357236\n",
      "batch 12/28 acc:0.90625, auc:0.9406492710113525, loss:0.34588796877164896\n",
      "batch 13/28 acc:0.921875, auc:0.7090892642736435, loss:0.28436859689007576\n",
      "batch 14/28 acc:0.96875, auc:0.9822510480880737, loss:0.15650076079811015\n",
      "batch 15/28 acc:0.875, auc:0.6910586059093475, loss:0.36882885186844305\n",
      "batch 16/28 acc:0.8125, auc:0.8980892300605774, loss:0.5340958714407975\n",
      "batch 17/28 acc:0.890625, auc:0.9405664205551147, loss:0.33124580253043234\n",
      "batch 18/28 acc:0.890625, auc:0.9372209906578064, loss:0.32309352736898234\n",
      "batch 19/28 acc:0.890625, auc:0.9106389284133911, loss:0.2954399472009932\n",
      "batch 20/28 acc:0.875, auc:0.6810151785612106, loss:0.43599449777431687\n",
      "batch 21/28 acc:0.828125, auc:0.9071159362792969, loss:0.46186128501449264\n",
      "epoch: 12/25, Test Loss: 1.9538210627456052, Test Acc: 0.4072577749536481, Test AUC: 0.4839490709657019\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 11 : 0.0001\n",
      "epoch 13.\n",
      "batch 0/28 acc:0.84375, auc:0.9153004884719849, loss:0.3646119114380326\n",
      "batch 1/28 acc:0.890625, auc:0.6938097923994064, loss:0.35173174721211886\n",
      "batch 2/28 acc:0.875, auc:0.9324728846549988, loss:0.3408194427381801\n",
      "batch 3/28 acc:0.890625, auc:0.6903096735477448, loss:0.28839824920157753\n",
      "batch 4/28 acc:0.953125, auc:0.9749729633331299, loss:0.2024188150860553\n",
      "batch 5/28 acc:0.890625, auc:0.6905972063541412, loss:0.28891829881480646\n",
      "batch 6/28 acc:0.875, auc:0.930984616279602, loss:0.33827611031530935\n",
      "batch 7/28 acc:0.859375, auc:0.9224737882614136, loss:0.3555160477266739\n",
      "batch 8/28 acc:0.828125, auc:0.8898376226425171, loss:0.47542200968754855\n",
      "batch 9/28 acc:0.859375, auc:0.9196910858154297, loss:0.5435495559149075\n",
      "batch 10/28 acc:0.9375, auc:0.9653950333595276, loss:0.30555889802246794\n",
      "batch 11/28 acc:0.90625, auc:0.9425325393676758, loss:0.2593380808400454\n",
      "batch 12/28 acc:0.859375, auc:0.9303769469261169, loss:0.38754391157895896\n",
      "batch 13/28 acc:0.78125, auc:0.8760154247283936, loss:0.49170454412855236\n",
      "batch 14/28 acc:0.875, auc:0.9369510412216187, loss:0.405900371251505\n",
      "batch 15/28 acc:0.84375, auc:0.9167014956474304, loss:0.33258235789904234\n",
      "batch 16/28 acc:0.90625, auc:0.9518406987190247, loss:0.20418520050429834\n",
      "batch 17/28 acc:0.90625, auc:0.9209064245223999, loss:0.3066350872390444\n",
      "batch 18/28 acc:0.875, auc:0.93049156665802, loss:0.35368169603111355\n",
      "batch 19/28 acc:0.84375, auc:0.9125635623931885, loss:0.55564094518774\n",
      "batch 20/28 acc:0.90625, auc:0.9503382444381714, loss:0.3593630550658986\n",
      "batch 21/28 acc:0.859375, auc:0.801235556602478, loss:0.3719911905183366\n",
      "epoch: 13/25, Test Loss: 2.0039875815240036, Test Acc: 0.44242311636074394, Test AUC: 0.48316302082755347\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 12 : 0.0001\n",
      "epoch 14.\n",
      "batch 0/28 acc:0.875, auc:0.68450728058815, loss:0.28874639134102154\n",
      "batch 1/28 acc:0.890625, auc:0.9410748481750488, loss:0.33207781914746437\n",
      "batch 2/28 acc:0.84375, auc:0.8601660132408142, loss:0.5324974627406842\n",
      "batch 3/28 acc:0.796875, auc:0.6436483263969421, loss:0.4976617040806559\n",
      "batch 4/28 acc:0.90625, auc:0.9452031850814819, loss:0.3656577606159317\n",
      "batch 5/28 acc:0.953125, auc:0.7240466177463531, loss:0.20603064145928585\n",
      "batch 6/28 acc:0.921875, auc:0.9533591270446777, loss:0.2526792660151216\n",
      "batch 7/28 acc:0.890625, auc:0.9105976819992065, loss:0.28171865318682165\n",
      "batch 8/28 acc:0.84375, auc:0.662043571472168, loss:0.30364952002114975\n",
      "batch 9/28 acc:0.921875, auc:0.9559553265571594, loss:0.27409327712237186\n",
      "batch 10/28 acc:0.9375, auc:0.7126061320304871, loss:0.27168542162490894\n",
      "batch 11/28 acc:0.890625, auc:0.9385020732879639, loss:0.30079280775305506\n",
      "batch 12/28 acc:0.921875, auc:0.9572832584381104, loss:0.2242274381073983\n",
      "batch 13/28 acc:0.890625, auc:0.942451000213623, loss:0.31041737605154385\n",
      "batch 14/28 acc:0.9375, auc:0.9588264226913452, loss:0.23778778071860884\n",
      "batch 15/28 acc:0.90625, auc:0.9440022706985474, loss:0.2722323776674216\n",
      "batch 16/28 acc:0.828125, auc:0.9092190861701965, loss:0.429175700676673\n",
      "batch 17/28 acc:0.859375, auc:0.8886435627937317, loss:0.2988532617360278\n",
      "batch 18/28 acc:0.890625, auc:0.68819560110569, loss:0.30909366394865856\n",
      "batch 19/28 acc:0.890625, auc:0.9323240518569946, loss:0.4737059348370103\n",
      "batch 20/28 acc:0.953125, auc:0.9731736183166504, loss:0.235988914511708\n",
      "batch 21/28 acc:0.921875, auc:0.9532461166381836, loss:0.2365004449312451\n",
      "epoch: 14/25, Test Loss: 2.1503881775914495, Test Acc: 0.48393565214484596, Test AUC: 0.47423727133057336\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 13 : 0.0001\n",
      "epoch 15.\n",
      "batch 0/28 acc:0.90625, auc:0.6964008212089539, loss:0.27223827907960185\n",
      "batch 1/28 acc:0.859375, auc:0.920356273651123, loss:0.41779826151352495\n",
      "batch 2/28 acc:0.890625, auc:0.9402496814727783, loss:0.30035286524110205\n",
      "batch 3/28 acc:0.921875, auc:0.9559153914451599, loss:0.2034713844008209\n",
      "batch 4/28 acc:0.84375, auc:0.9134640097618103, loss:0.42897186751937966\n",
      "batch 5/28 acc:0.953125, auc:0.9730961322784424, loss:0.17305490827102687\n",
      "batch 6/28 acc:0.859375, auc:0.9201151132583618, loss:0.35926302516128317\n",
      "batch 7/28 acc:0.921875, auc:0.9195199012756348, loss:0.19978382116980242\n",
      "batch 8/28 acc:0.875, auc:0.9333004355430603, loss:0.3675247957241652\n",
      "batch 9/28 acc:0.90625, auc:0.9484184980392456, loss:0.29859253638254124\n",
      "batch 10/28 acc:0.828125, auc:0.8948949575424194, loss:0.43237604859572\n",
      "batch 11/28 acc:0.859375, auc:0.662086233496666, loss:0.43559008619601247\n",
      "batch 12/28 acc:0.921875, auc:0.7094733566045761, loss:0.23385020860320083\n",
      "batch 13/28 acc:0.921875, auc:0.9187685251235962, loss:0.3057297938374468\n",
      "batch 14/28 acc:0.859375, auc:0.8175092935562134, loss:0.3437639496028737\n",
      "batch 15/28 acc:0.90625, auc:0.7027526646852493, loss:0.2578471492206518\n",
      "batch 16/28 acc:0.84375, auc:0.8462383151054382, loss:0.4639540697250464\n",
      "batch 17/28 acc:0.90625, auc:0.873684287071228, loss:0.2667505722791734\n",
      "batch 18/28 acc:0.90625, auc:0.943556547164917, loss:0.26080199195564546\n",
      "batch 19/28 acc:0.890625, auc:0.6900675296783447, loss:0.3227344815874922\n",
      "batch 20/28 acc:0.828125, auc:0.8967849612236023, loss:0.39265460492813453\n",
      "batch 21/28 acc:0.890625, auc:0.9405293464660645, loss:0.28817275615747917\n",
      "epoch: 15/25, Test Loss: 2.072598353467338, Test Acc: 0.42915575761762076, Test AUC: 0.4550707665356723\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 14 : 0.0001\n",
      "epoch 16.\n",
      "batch 0/28 acc:0.875, auc:0.6806835979223251, loss:0.35041654109393505\n",
      "batch 1/28 acc:0.890625, auc:0.929084300994873, loss:0.36896467705869174\n",
      "batch 2/28 acc:0.90625, auc:0.9493227601051331, loss:0.27010577838871075\n",
      "batch 3/28 acc:0.890625, auc:0.9119710922241211, loss:0.3743690637865793\n",
      "batch 4/28 acc:0.921875, auc:0.95610511302948, loss:0.2353424441085359\n",
      "batch 5/28 acc:0.890625, auc:0.904270589351654, loss:0.23634042843053749\n",
      "batch 6/28 acc:0.875, auc:0.9346275925636292, loss:0.2875973959243012\n",
      "batch 7/28 acc:0.84375, auc:0.6766935437917709, loss:0.34499747166248085\n",
      "batch 8/28 acc:0.890625, auc:0.9355229139328003, loss:0.41381376205711007\n",
      "batch 9/28 acc:0.90625, auc:0.9490235447883606, loss:0.27457820474307937\n",
      "batch 10/28 acc:0.953125, auc:0.9676339030265808, loss:0.2631830007537097\n",
      "batch 11/28 acc:0.9375, auc:0.8431111574172974, loss:0.1598811318374853\n",
      "batch 12/28 acc:0.9375, auc:0.9599392414093018, loss:0.20119553030542647\n",
      "batch 13/28 acc:0.9375, auc:0.9645308256149292, loss:0.2484918941077794\n",
      "batch 14/28 acc:0.84375, auc:0.8787015676498413, loss:0.38140793314664734\n",
      "batch 15/28 acc:0.9375, auc:0.963154673576355, loss:0.2311778751367619\n",
      "batch 16/28 acc:0.859375, auc:0.8094767332077026, loss:0.39991842722403703\n",
      "batch 17/28 acc:0.9375, auc:0.9619048237800598, loss:0.22585226744763265\n",
      "batch 18/28 acc:0.84375, auc:0.7964826822280884, loss:0.31965587551682084\n",
      "batch 19/28 acc:0.9375, auc:0.9650853276252747, loss:0.21421831189633167\n",
      "batch 20/28 acc:0.921875, auc:0.9535329341888428, loss:0.2572913582795504\n",
      "batch 21/28 acc:0.9375, auc:0.9643087387084961, loss:0.12547415114981675\n",
      "epoch: 16/25, Test Loss: 2.104202179093619, Test Acc: 0.41212393252504603, Test AUC: 0.5098684467375278\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 15 : 0.0001\n",
      "epoch 17.\n",
      "batch 0/28 acc:0.921875, auc:0.9555026292800903, loss:0.22105225210458457\n",
      "batch 1/28 acc:0.921875, auc:0.9586775302886963, loss:0.22401918342607985\n",
      "batch 2/28 acc:0.921875, auc:0.7078607827425003, loss:0.271092572003095\n",
      "batch 3/28 acc:0.96875, auc:0.7336128354072571, loss:0.12909789852549558\n",
      "batch 4/28 acc:0.953125, auc:0.972886323928833, loss:0.21026722043116308\n",
      "batch 5/28 acc:0.875, auc:0.6762919723987579, loss:0.3512407948653866\n",
      "batch 6/28 acc:0.96875, auc:0.9826730489730835, loss:0.17681078680777063\n",
      "batch 7/28 acc:0.890625, auc:0.9386675953865051, loss:0.3736120681015862\n",
      "batch 8/28 acc:0.921875, auc:0.9365972280502319, loss:0.26351163310371106\n",
      "batch 9/28 acc:0.875, auc:0.8731301426887512, loss:0.3680279357237737\n",
      "batch 10/28 acc:0.921875, auc:0.7093885242938995, loss:0.21455912546608147\n",
      "batch 11/28 acc:0.90625, auc:0.9415520429611206, loss:0.30608927270367303\n",
      "batch 12/28 acc:0.890625, auc:0.6917701810598373, loss:0.3103277589428046\n",
      "batch 13/28 acc:0.921875, auc:0.9588689804077148, loss:0.22712053092858753\n",
      "batch 14/28 acc:0.96875, auc:0.9827201962471008, loss:0.18171113228066282\n",
      "batch 15/28 acc:0.96875, auc:0.9824175834655762, loss:0.12121716667951432\n",
      "batch 16/28 acc:0.890625, auc:0.9343609809875488, loss:0.32477512537639086\n",
      "batch 17/28 acc:0.90625, auc:0.9442297220230103, loss:0.23433229053760485\n",
      "batch 18/28 acc:0.96875, auc:0.9847664833068848, loss:0.10368338960890355\n",
      "batch 19/28 acc:0.859375, auc:0.9272865056991577, loss:0.35005541323027956\n",
      "batch 20/28 acc:0.9375, auc:0.9651091694831848, loss:0.24240877060609378\n",
      "batch 21/28 acc:0.859375, auc:0.9266340732574463, loss:0.31446649999074294\n",
      "epoch: 17/25, Test Loss: 2.024739395319871, Test Acc: 0.47312087872694664, Test AUC: 0.4909601177681576\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 16 : 0.0001\n",
      "epoch 18.\n",
      "batch 0/28 acc:0.9375, auc:0.9599405527114868, loss:0.18886314268596038\n",
      "batch 1/28 acc:0.90625, auc:0.9481629133224487, loss:0.2547126817324282\n",
      "batch 2/28 acc:0.953125, auc:0.9760909080505371, loss:0.21488571073641793\n",
      "batch 3/28 acc:0.953125, auc:0.9729786515235901, loss:0.15360719579286197\n",
      "batch 4/28 acc:0.9375, auc:0.9657148122787476, loss:0.1635580604613267\n",
      "batch 5/28 acc:0.921875, auc:0.9553177356719971, loss:0.17713916261024565\n",
      "batch 6/28 acc:0.9375, auc:0.9629076719284058, loss:0.19557162660960614\n",
      "batch 7/28 acc:0.9375, auc:0.9635010361671448, loss:0.1492034913242719\n",
      "batch 8/28 acc:0.9375, auc:0.9670320749282837, loss:0.2492043102682473\n",
      "batch 9/28 acc:0.984375, auc:0.9658430218696594, loss:0.11219563377338204\n",
      "batch 10/28 acc:0.984375, auc:0.992931604385376, loss:0.0951485861990875\n",
      "batch 11/28 acc:0.90625, auc:0.8303152322769165, loss:0.25003925119014525\n",
      "batch 12/28 acc:0.90625, auc:0.9473431706428528, loss:0.2870019049143018\n",
      "batch 13/28 acc:0.96875, auc:0.981680154800415, loss:0.1341060663405731\n",
      "batch 14/28 acc:0.890625, auc:0.9377195835113525, loss:0.4723177169981625\n",
      "batch 15/28 acc:0.90625, auc:0.9334139227867126, loss:0.2498058848598248\n",
      "batch 16/28 acc:0.84375, auc:0.9070049524307251, loss:0.3960785787882486\n",
      "batch 17/28 acc:0.953125, auc:0.9738436937332153, loss:0.1490233860862915\n",
      "batch 18/28 acc:0.828125, auc:0.8232421278953552, loss:0.4286611426646729\n",
      "batch 19/28 acc:0.96875, auc:0.8654303550720215, loss:0.14843368404524426\n",
      "batch 20/28 acc:0.875, auc:0.9387896060943604, loss:0.3514531222109927\n",
      "batch 21/28 acc:0.9375, auc:0.7132076323032379, loss:0.24307748999857637\n",
      "epoch: 18/25, Test Loss: 2.1638873987583325, Test Acc: 0.4515651510507311, Test AUC: 0.4849576445465738\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 17 : 0.0001\n",
      "epoch 19.\n",
      "batch 0/28 acc:0.921875, auc:0.8417031764984131, loss:0.3111056527986733\n",
      "batch 1/28 acc:0.875, auc:0.8778424263000488, loss:0.25410131113970635\n",
      "batch 2/28 acc:0.9375, auc:0.9608045816421509, loss:0.13265792819697697\n",
      "batch 3/28 acc:0.953125, auc:0.9521428346633911, loss:0.22535939828650342\n",
      "batch 4/28 acc:0.953125, auc:0.9702506065368652, loss:0.14552387058238914\n",
      "batch 5/28 acc:0.921875, auc:0.705661416053772, loss:0.1691159811580576\n",
      "batch 6/28 acc:0.90625, auc:0.9482545256614685, loss:0.30552490894078943\n",
      "batch 7/28 acc:0.875, auc:0.9290902614593506, loss:0.3362921488439099\n",
      "batch 8/28 acc:0.953125, auc:0.970011293888092, loss:0.19218195991122755\n",
      "batch 9/28 acc:0.921875, auc:0.9560908079147339, loss:0.2522814980583017\n",
      "batch 10/28 acc:0.90625, auc:0.9471480846405029, loss:0.19810190963723073\n",
      "batch 11/28 acc:0.921875, auc:0.9532790184020996, loss:0.16921750590302054\n",
      "batch 12/28 acc:0.953125, auc:0.9694926738739014, loss:0.22129007580578275\n",
      "batch 13/28 acc:0.984375, auc:0.9928703904151917, loss:0.07849048800440528\n",
      "batch 14/28 acc:0.96875, auc:0.9832305312156677, loss:0.1521832011850961\n",
      "batch 15/28 acc:0.9375, auc:0.9680500030517578, loss:0.16960405581864535\n",
      "batch 16/28 acc:0.828125, auc:0.9022607803344727, loss:0.5173043521463185\n",
      "batch 17/28 acc:0.859375, auc:0.864276647567749, loss:0.4115559009330667\n",
      "batch 18/28 acc:0.921875, auc:0.9549397826194763, loss:0.21627994970356212\n",
      "batch 19/28 acc:0.921875, auc:0.9579077959060669, loss:0.1782455653393029\n",
      "batch 20/28 acc:0.90625, auc:0.9432277679443359, loss:0.29422517895301326\n",
      "batch 21/28 acc:0.921875, auc:0.9211685657501221, loss:0.2028122281536966\n",
      "epoch: 19/25, Test Loss: 2.1999193333381855, Test Acc: 0.4446778365268242, Test AUC: 0.47724452648650517\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 18 : 0.0001\n",
      "epoch 20.\n",
      "batch 0/28 acc:0.96875, auc:0.7322416007518768, loss:0.07903336870074895\n",
      "batch 1/28 acc:0.84375, auc:0.6624188274145126, loss:0.4787582759268574\n",
      "batch 2/28 acc:0.96875, auc:0.980043888092041, loss:0.14358050129513344\n",
      "batch 3/28 acc:0.9375, auc:0.9337774515151978, loss:0.23902589700492172\n",
      "batch 4/28 acc:0.921875, auc:0.9555951356887817, loss:0.25099354296066423\n",
      "batch 5/28 acc:0.9375, auc:0.9670048356056213, loss:0.19128665711180304\n",
      "batch 6/28 acc:0.890625, auc:0.9337208271026611, loss:0.27449230682162806\n",
      "batch 7/28 acc:0.984375, auc:0.9347222447395325, loss:0.06689833075131446\n",
      "batch 8/28 acc:0.984375, auc:0.9925925731658936, loss:0.10480778058649776\n",
      "batch 9/28 acc:0.921875, auc:0.9205048084259033, loss:0.20911809356034894\n",
      "batch 10/28 acc:0.90625, auc:0.9459013938903809, loss:0.20444478832104096\n",
      "batch 11/28 acc:0.875, auc:0.9233058094978333, loss:0.3156049594523722\n",
      "batch 12/28 acc:0.890625, auc:0.9388999342918396, loss:0.2240616643463227\n",
      "batch 13/28 acc:0.859375, auc:0.7987610101699829, loss:0.33226632669613565\n",
      "batch 14/28 acc:0.96875, auc:0.9823868870735168, loss:0.1268328933760614\n",
      "batch 15/28 acc:0.9375, auc:0.9098077416419983, loss:0.22253380336127293\n",
      "batch 16/28 acc:0.890625, auc:0.9282270073890686, loss:0.3179574913671104\n",
      "batch 17/28 acc:0.953125, auc:0.9734361171722412, loss:0.21478502663271026\n",
      "batch 18/28 acc:0.9375, auc:0.9616329669952393, loss:0.23760520931714513\n",
      "batch 19/28 acc:0.90625, auc:0.703138530254364, loss:0.24628095214460188\n",
      "batch 20/28 acc:0.96875, auc:0.9635041952133179, loss:0.11699476281026477\n",
      "batch 21/28 acc:0.9375, auc:0.9608113765716553, loss:0.27493522402325254\n",
      "epoch: 20/25, Test Loss: 2.124227024986616, Test Acc: 0.46498390878898993, Test AUC: 0.5005446123805913\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 19 : 0.0001\n",
      "epoch 21.\n",
      "batch 0/28 acc:0.984375, auc:0.9908430576324463, loss:0.07725426484390141\n",
      "batch 1/28 acc:0.9375, auc:0.9298157691955566, loss:0.17093163981547832\n",
      "batch 2/28 acc:0.96875, auc:0.9818181991577148, loss:0.1442038180523184\n",
      "batch 3/28 acc:0.921875, auc:0.9511382579803467, loss:0.21505264089761056\n",
      "batch 4/28 acc:0.984375, auc:0.9917140007019043, loss:0.0865428730177058\n",
      "batch 5/28 acc:0.921875, auc:0.957305908203125, loss:0.1604390017827768\n",
      "batch 6/28 acc:0.890625, auc:0.9403084516525269, loss:0.3403725286656112\n",
      "batch 7/28 acc:0.9375, auc:0.9646803140640259, loss:0.1636001060318648\n",
      "batch 8/28 acc:0.90625, auc:0.9427489042282104, loss:0.2946140846759917\n",
      "batch 9/28 acc:0.953125, auc:0.9761624336242676, loss:0.12471554639262195\n",
      "batch 10/28 acc:0.9375, auc:0.9619407653808594, loss:0.1660943005451685\n",
      "batch 11/28 acc:0.9375, auc:0.9662637114524841, loss:0.16159687226146957\n",
      "batch 12/28 acc:0.9375, auc:0.9636356830596924, loss:0.19740691194985516\n",
      "batch 13/28 acc:0.953125, auc:0.9736623167991638, loss:0.15114485692236812\n",
      "batch 14/28 acc:0.9375, auc:0.9660466313362122, loss:0.19515859948816683\n",
      "batch 15/28 acc:0.921875, auc:0.9272041320800781, loss:0.18592790873178444\n",
      "batch 16/28 acc:0.96875, auc:0.9809393286705017, loss:0.1483074838534435\n",
      "batch 17/28 acc:0.9375, auc:0.9587793946266174, loss:0.35817236087954996\n",
      "batch 18/28 acc:0.96875, auc:0.9837906956672668, loss:0.12459866163101196\n",
      "batch 19/28 acc:0.9375, auc:0.9669445753097534, loss:0.12291804592827127\n",
      "batch 20/28 acc:0.953125, auc:0.9759676456451416, loss:0.16823778492351948\n",
      "batch 21/28 acc:0.9375, auc:0.9662471413612366, loss:0.23575914720588287\n",
      "epoch: 21/25, Test Loss: 2.427564539016254, Test Acc: 0.4435525657868572, Test AUC: 0.48306688971140166\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 20 : 0.0001\n",
      "epoch 22.\n",
      "batch 0/28 acc:0.9375, auc:0.9585708975791931, loss:0.20267226696552143\n",
      "batch 1/28 acc:0.921875, auc:0.9190781116485596, loss:0.29647911893023604\n",
      "batch 2/28 acc:0.921875, auc:0.9543728828430176, loss:0.19628463389355488\n",
      "batch 3/28 acc:0.984375, auc:0.9898061752319336, loss:0.09459254846246523\n",
      "batch 4/28 acc:0.921875, auc:0.9547421932220459, loss:0.18288134677709422\n",
      "batch 5/28 acc:0.890625, auc:0.9289963245391846, loss:0.301810159799329\n",
      "batch 6/28 acc:0.90625, auc:0.9489960074424744, loss:0.23433120354338577\n",
      "batch 7/28 acc:0.96875, auc:0.9854191541671753, loss:0.16074877241101104\n",
      "batch 8/28 acc:0.921875, auc:0.9478380084037781, loss:0.19753305520529807\n",
      "batch 9/28 acc:0.984375, auc:0.986204981803894, loss:0.13492186086836\n",
      "batch 10/28 acc:0.96875, auc:0.9822410345077515, loss:0.18456705569258602\n",
      "batch 11/28 acc:0.96875, auc:0.9821857213973999, loss:0.1978054575216318\n",
      "batch 12/28 acc:0.921875, auc:0.7065675556659698, loss:0.21923831894627455\n",
      "batch 13/28 acc:0.921875, auc:0.9611762166023254, loss:0.18477507329359355\n",
      "batch 14/28 acc:0.921875, auc:0.9569277763366699, loss:0.2045705927693433\n",
      "batch 15/28 acc:0.921875, auc:0.9511318206787109, loss:0.26973542447430354\n",
      "batch 16/28 acc:0.953125, auc:0.9757765531539917, loss:0.14734341628787462\n",
      "batch 17/28 acc:0.90625, auc:0.9507572650909424, loss:0.3178398823812678\n",
      "batch 18/28 acc:0.84375, auc:0.9130681753158569, loss:0.36103524817704447\n",
      "batch 19/28 acc:0.96875, auc:0.9833475351333618, loss:0.13004095130025917\n",
      "batch 20/28 acc:0.96875, auc:0.9254862666130066, loss:0.12201744177458806\n",
      "batch 21/28 acc:0.96875, auc:0.7365110069513321, loss:0.14181342634081595\n",
      "epoch: 22/25, Test Loss: 2.311158509359562, Test Acc: 0.3983183979626247, Test AUC: 0.4976078017868779\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 21 : 0.0001\n",
      "epoch 23.\n",
      "batch 0/28 acc:0.90625, auc:0.9223909974098206, loss:0.26009465705537593\n",
      "batch 1/28 acc:0.9375, auc:0.9672790169715881, loss:0.2769230557463871\n",
      "batch 2/28 acc:0.953125, auc:0.970267653465271, loss:0.111961516060191\n",
      "batch 3/28 acc:0.96875, auc:0.984137773513794, loss:0.13579459621275092\n",
      "batch 4/28 acc:0.96875, auc:0.9102564454078674, loss:0.14622717066749225\n",
      "batch 5/28 acc:0.921875, auc:0.9521930813789368, loss:0.22717311478604074\n",
      "batch 6/28 acc:0.984375, auc:0.9882305264472961, loss:0.08165998571351096\n",
      "batch 7/28 acc:0.953125, auc:0.9737592935562134, loss:0.1392512374142001\n",
      "batch 8/28 acc:0.890625, auc:0.93669593334198, loss:0.3098918586188102\n",
      "batch 9/28 acc:0.96875, auc:0.9563873410224915, loss:0.0599546107638389\n",
      "batch 10/28 acc:0.953125, auc:0.9751343727111816, loss:0.17875229249083802\n",
      "batch 11/28 acc:0.90625, auc:0.9509469270706177, loss:0.2075140331344656\n",
      "batch 12/28 acc:0.953125, auc:0.9738128781318665, loss:0.11878668120521141\n",
      "batch 13/28 acc:0.9375, auc:0.957783579826355, loss:0.18001326176224808\n",
      "batch 14/28 acc:0.90625, auc:0.9479734897613525, loss:0.20709773229714656\n",
      "batch 15/28 acc:0.921875, auc:0.9436274766921997, loss:0.23013160637487573\n",
      "batch 16/28 acc:0.953125, auc:0.9749059677124023, loss:0.211523032573659\n",
      "batch 17/28 acc:0.859375, auc:0.9166780710220337, loss:0.3429830352303753\n",
      "batch 18/28 acc:0.953125, auc:0.973987340927124, loss:0.22168681539742252\n",
      "batch 19/28 acc:0.96875, auc:0.9572864770889282, loss:0.08264772464966086\n",
      "batch 20/28 acc:0.96875, auc:0.9826118350028992, loss:0.09493351933227867\n",
      "batch 21/28 acc:0.96875, auc:0.982352077960968, loss:0.12722719609421063\n",
      "epoch: 23/25, Test Loss: 2.233285154608852, Test Acc: 0.4651092036691575, Test AUC: 0.4850026562132619\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 22 : 0.0001\n",
      "epoch 24.\n",
      "batch 0/28 acc:0.9375, auc:0.9595222473144531, loss:0.17898375249963816\n",
      "batch 1/28 acc:0.984375, auc:0.9907581210136414, loss:0.1596787684776686\n",
      "batch 2/28 acc:0.953125, auc:0.9737836122512817, loss:0.1283877953009238\n",
      "batch 3/28 acc:0.9375, auc:0.9672166705131531, loss:0.23196305172162113\n",
      "batch 4/28 acc:0.953125, auc:0.9690437316894531, loss:0.12839954600079295\n",
      "batch 5/28 acc:0.9375, auc:0.9625641703605652, loss:0.18305490885003906\n",
      "batch 6/28 acc:0.875, auc:0.9248619079589844, loss:0.3218998095515033\n",
      "batch 7/28 acc:0.9375, auc:0.9649950265884399, loss:0.14222777326405733\n",
      "batch 8/28 acc:0.96875, auc:0.9826570749282837, loss:0.09737397964329375\n",
      "batch 9/28 acc:0.984375, auc:0.9900426864624023, loss:0.0769069770802262\n",
      "batch 10/28 acc:0.859375, auc:0.9110034704208374, loss:0.3132376816147646\n",
      "batch 11/28 acc:0.90625, auc:0.94394451379776, loss:0.1645352068274022\n",
      "batch 12/28 acc:0.921875, auc:0.7097537964582443, loss:0.17470294825425015\n",
      "batch 13/28 acc:0.953125, auc:0.7225430607795715, loss:0.1718774759599455\n",
      "batch 14/28 acc:0.96875, auc:0.9834253191947937, loss:0.07997089878671915\n",
      "batch 15/28 acc:0.921875, auc:0.9541527032852173, loss:0.24761863566083342\n",
      "batch 16/28 acc:0.9375, auc:0.7112126350402832, loss:0.1675151463162763\n",
      "batch 17/28 acc:0.953125, auc:0.7257456034421921, loss:0.11179541228709766\n",
      "batch 18/28 acc:0.9375, auc:0.9595642685890198, loss:0.12349379635473312\n",
      "batch 19/28 acc:0.921875, auc:0.9562831521034241, loss:0.25799177826137054\n",
      "batch 20/28 acc:0.953125, auc:0.9727550745010376, loss:0.18015435990719197\n",
      "batch 21/28 acc:0.9375, auc:0.9257730841636658, loss:0.21808405640688022\n",
      "epoch: 24/25, Test Loss: 2.1069936600024293, Test Acc: 0.49306506166565, Test AUC: 0.5090194219215349\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 23 : 0.0001\n",
      "epoch 25.\n",
      "batch 0/28 acc:0.90625, auc:0.9393633008003235, loss:0.26298037289250686\n",
      "batch 1/28 acc:0.96875, auc:0.980582594871521, loss:0.13198023238365408\n",
      "batch 2/28 acc:0.96875, auc:0.9466873407363892, loss:0.07928364980458724\n",
      "batch 3/28 acc:0.9375, auc:0.9622600078582764, loss:0.16230167403396933\n",
      "batch 4/28 acc:0.9375, auc:0.963666558265686, loss:0.19080724372349778\n",
      "batch 5/28 acc:0.953125, auc:0.9768182039260864, loss:0.17479006225730132\n",
      "batch 6/28 acc:0.90625, auc:0.9292219877243042, loss:0.24694595803980235\n",
      "batch 7/28 acc:0.90625, auc:0.9438481330871582, loss:0.26342096635977796\n",
      "batch 8/28 acc:0.9375, auc:0.8334894180297852, loss:0.19605659951189125\n",
      "batch 9/28 acc:0.96875, auc:0.9823588728904724, loss:0.1735654438808183\n",
      "batch 10/28 acc:0.984375, auc:0.9907011985778809, loss:0.09499451563299344\n",
      "batch 11/28 acc:0.9375, auc:0.7148071676492691, loss:0.2561446941561343\n",
      "batch 12/28 acc:0.9375, auc:0.9615370035171509, loss:0.14671319901810875\n",
      "batch 13/28 acc:0.9375, auc:0.9632076621055603, loss:0.21174610853852016\n",
      "batch 14/28 acc:0.953125, auc:0.9518173933029175, loss:0.18928998360356175\n",
      "batch 15/28 acc:0.953125, auc:0.723283514380455, loss:0.140634646844358\n",
      "batch 16/28 acc:0.96875, auc:0.9783307909965515, loss:0.1284710625484493\n",
      "batch 17/28 acc:0.9375, auc:0.9282369613647461, loss:0.21638502180934438\n",
      "batch 18/28 acc:0.90625, auc:0.9498341083526611, loss:0.27186849914227906\n",
      "batch 19/28 acc:0.9375, auc:0.9646831154823303, loss:0.20503568734310207\n",
      "batch 20/28 acc:0.9375, auc:0.9642977714538574, loss:0.2527692608266817\n",
      "batch 21/28 acc:0.96875, auc:0.9813735485076904, loss:0.08364981325054188\n",
      "epoch: 25/25, Test Loss: 2.3932304389150802, Test Acc: 0.4494269667717942, Test AUC: 0.4792483062906699\n",
      "\n",
      "\n",
      "==============================\n",
      "learning rate epoch 24 : 0.0001\n"
     ]
    }
   ],
   "source": [
    "train(model, params, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bdd7dba6-2f99-406c-93f1-08ef8300d234",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "  )\n",
       "  (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6-7): 2 x MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9-10): 2 x MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12-14): 3 x MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=4, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65580d05-4446-4625-bf5d-39f12584621a",
   "metadata": {},
   "source": [
    "### Save Model State "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05a47819-ec7f-4f75-aba5-84f515e70730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37905a83-2063-4b0e-8601-d0d82e338a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './weights/model_weight_efficientB0_230513(2).pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2042f8d8-4612-4cb8-abd7-f0ffd8f01b07",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc00b362-b9ff-4502-870a-cf075ca6ea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "pred_list.append(['image_id', 'healthy', 'multiple_diseases', 'rust', 'scab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3bbe0d55-c976-45dc-b035-e0078ff1c2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Test_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id\n",
       "0   Test_0\n",
       "1   Test_1\n",
       "2   Test_2\n",
       "3   Test_3\n",
       "4   Test_4"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_data = pd.read_csv('test.csv')\n",
    "submission_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "58118079-5d5f-4381-997e-69884ad2e491",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_path = '/home/dmsai2/Desktop/AI-Study/PlantPathology2020/images/Submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "144ce262-2b4d-4b3c-aa10-ed9c2a1cd200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc0b4566-34f9-4612-aeda-0ab132a4a5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1821 [00:00<?, ?it/s]/root/anaconda3/envs/hackthon2/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████| 1821/1821 [01:21<00:00, 22.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(submission_data))):\n",
    "    img_title = submission_data.iloc[i, 0]\n",
    "    img_dir = img_title + '.jpg'\n",
    "    # print(op(submission_path, img_dir))\n",
    "    \n",
    "    image = read_image(op(submission_path, img_dir))\n",
    "    image = image.type(torch.FloatTensor)\n",
    "    img = image / 255.\n",
    "    img = tf(img)\n",
    "    img = img.reshape(1, 3, 224, 224)\n",
    "    inputs = img.to(device)\n",
    "    # print(inputs.shape)\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    ans = [img_title, 0, 0, 0, 0]\n",
    "    ans[int(preds) + 1] = 1\n",
    "    pred_list.append(ans)\n",
    "print(len(pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "be1899fd-d909-4f6f-856b-bbad34ec5538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_id</td>\n",
       "      <td>healthy</td>\n",
       "      <td>multiple_diseases</td>\n",
       "      <td>rust</td>\n",
       "      <td>scab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Test_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0        1                  2     3     4\n",
       "0  image_id  healthy  multiple_diseases  rust  scab\n",
       "1    Test_0        0                  0     1     0\n",
       "2    Test_1        1                  0     0     0\n",
       "3    Test_2        0                  0     0     1\n",
       "4    Test_3        0                  0     1     0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.DataFrame(pred_list)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e5995e9-b1b5-46bc-8666-cdb00d001816",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('./submission/submission_efnet2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3aaec6-e5b2-4371-811b-9ec7ef083edb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
